{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is prepare the raw text data for ingestion by the LSTM model. There are several steps.\n",
    "First, the raw data is read into two lists, the original and compressed test (\\_read_leg_compressions)\n",
    "In the extraction phase:\n",
    "\n",
    " - the text is tokenzised\n",
    " - a vocabulary file is built\n",
    " - labels are derived\n",
    " \n",
    " \n",
    "The first step of the LSTM model is to lookup word2vec embeddings for the text inputs. Rather than provide the model with all of the word2vec embeddings (which are around 3.5GB when held in memory), we extract the vocabulary of the text inputs and only provide the embeddings associated with that vocabulary.\n",
    "\n",
    "The Word2Vec model can be downloaded from <https://code.google.com/archive/p/word2vec/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import spacy\n",
    "import json\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md', disable=['parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import codecs\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the inputs\n",
    "\n",
    "def _read_leg_compressions(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        full_sents = []\n",
    "        compressed = []\n",
    "        for i in data:\n",
    "            full_sents.append(data[i]['full_text'])\n",
    "            compressed.append(data[i]['compressed_text'])\n",
    "        return full_sents, compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the raw text inputs\n",
    "\n",
    "def extract(full_sents, compressed, text_out, labels_out, vocab_out, text_whole):\n",
    "    with open(text_out, \"w\", encoding=\"utf-8\") as text_out:\n",
    "        with open(labels_out, \"w\", encoding=\"utf-8\") as labels_out:\n",
    "            with open(text_whole, \"w\", encoding=\"utf-8\") as text_whole:\n",
    "                \n",
    "                vocab = []\n",
    "                vocab.append(\"<unk>\")\n",
    "                vocab.append(\"</S>\")\n",
    "                vocab.append(\"</l>\")\n",
    "                vocab.append(\"<l>\")\n",
    "                bad_parse_count = 0\n",
    "                bad_inputs = []\n",
    "                \n",
    "                for i in range(len(full_sents)):\n",
    "                    text_data = []\n",
    "                    label_data = []\n",
    "                    trace = 0\n",
    "                    doc_whole = nlp(full_sents[i])\n",
    "                    doc_compressed = nlp(compressed[i])\n",
    "                    for token in doc_whole:\n",
    "                        text_data.append(token.text)\n",
    "                        vocab.append(token.text)\n",
    "                        if trace < len(doc_compressed):\n",
    "                            if token.text == doc_compressed[trace].text:\n",
    "                                trace += 1\n",
    "                                label_data.append(\"1\")\n",
    "                            else:\n",
    "                                label_data.append(\"0\")\n",
    "                        else:\n",
    "                            label_data.append(\"0\")\n",
    "\n",
    "                    # Check if labels is empty... If it is, see if it's because the first word of the compressed form is not \n",
    "                    # capitalised in the original form. If that's the case, check against the lowercase form of the first word \n",
    "                    # of the compression\n",
    "\n",
    "                    if '1' not in label_data:\n",
    "                        if (doc_compressed[0] not in doc_whole) and (doc_compressed[0].lower_ in doc_whole.text):\n",
    "                            label_data = []\n",
    "                            trace = 0\n",
    "                            for token in doc_whole:\n",
    "                                if trace == 0:\n",
    "                                    if token.text == doc_compressed[0].lower_:\n",
    "                                        trace += 1\n",
    "                                        label_data.append(\"1\")\n",
    "                                    else:\n",
    "                                        label_data.append(\"0\")\n",
    "                                elif 0 < trace < len(doc_compressed):\n",
    "                                    if token.text == doc_compressed[trace].text:\n",
    "                                        trace += 1\n",
    "                                        label_data.append(\"1\")\n",
    "                                    else:\n",
    "                                        label_data.append(\"0\")\n",
    "                                else:\n",
    "                                    label_data.append(\"0\")\n",
    "\n",
    "                    # If labels is still empty, move on (but count it as a missed)\n",
    "\n",
    "                    if '1' in label_data:\n",
    "                        text_out.write(\" \".join(text_data) + \"\\n\")\n",
    "                        labels_out.write(\" \".join(label_data) + \"\\n\")\n",
    "                        text_whole.write(doc_whole.text + \"\\n\")\n",
    "                    else:\n",
    "                        bad_parse_count += 1\n",
    "                        print(bad_parse_count)\n",
    "                        bad_inputs.append(full_sents[i])\n",
    "                        print(\"Bad index:\")\n",
    "                        print(i)\n",
    "                    if len(label_data) != len(text_data):\n",
    "                        print(\"Bugger\")\n",
    "                        print(i)\n",
    "                counter = collections.Counter(vocab)\n",
    "                with open(vocab_out, \"w\", encoding=\"utf-8\") as vocab_out:\n",
    "                    for item in counter.keys():\n",
    "                        vocab_out.write(item + \"\\n\")\n",
    "    return bad_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and extract train, val and test sets (plus targetted train and val sets)\n",
    "\n",
    "full_sents, compressed = _read_leg_compressions(\"\") # Data path\n",
    "extract(full_sents, compressed, \"leg_train_text.txt\", \n",
    "        \"leg_train_label.txt\", \"leg_train_vocab.txt\", \"leg_train_original.txt\")\n",
    "\n",
    "full_sents, compressed = _read_leg_compressions(\"\") # Data path\n",
    "extract(full_sents, compressed, \"leg_val_text.txt\", \n",
    "        \"leg_val_label.txt\", \"leg_val_vocab.txt\", \"leg_val_original.txt\")\n",
    "\n",
    "full_sents, compressed = _read_leg_compressions(\"\") # Data path\n",
    "extract(full_sents, compressed, \"leg_test_text.txt\", \n",
    "        \"leg_test_label.txt\", \"leg_test_vocab.txt\", \"leg_test_original.txt\")\n",
    "\n",
    "full_sents, compressed = _read_leg_compressions(\"\") # Data path\n",
    "extract(full_sents, compressed, \"leg_train_targetted_text.txt\", \n",
    "        \"leg_train_targetted_label.txt\", \"leg_train_targetted_vocab.txt\", \"leg_train_targetted_original.txt\")\n",
    "\n",
    "full_sents, compressed = _read_leg_compressions(\"\") # Data path\n",
    "extract(full_sents, compressed, \"leg_val_targetted_text.txt\", \n",
    "        \"leg_val_targetted_label.txt\", \"leg_val_targetted_vocab.txt\", \"leg_val_targetted_original.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience, combine all vocab files.\n",
    "\n",
    "vocab_files = [\"leg_train_vocab.txt\", \n",
    "               \"leg_val_vocab.txt\",\n",
    "               \"leg_test_vocab.txt\",\n",
    "               \"leg_train_targetted_vocab.txt\",\n",
    "               \"leg_val_targetted_vocab.txt\"]\n",
    "\n",
    "total_vocab = []\n",
    "for file in vocab_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for word in f:\n",
    "            total_vocab.append(word)\n",
    "\n",
    "counter = collections.Counter(total_vocab)\n",
    "\n",
    "with open(\"total_vocab.txt\", 'w', encoding='utf-8') as f:        \n",
    "    for item in counter.keys():\n",
    "        f.write(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word2vec model\n",
    "\n",
    "word2vec_path = \"\" # Path to word2vec model\n",
    "w2v_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the data vocab and look-up embedding for each word. If an embedding does not exist, add to out_of_vocab list\n",
    "# (for tracking purposes only). Write embeddings to embedding file. Also write embeddings vocab.\n",
    "\n",
    "out_of_vocab = []\n",
    "with open(\"total_vocab.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    with open(\"leg_embeddings.txt\", \"w\", encoding=\"utf-8\") as e_file:\n",
    "        with open(\"leg_embeddings_vocab.txt\", \"w\", encoding=\"utf-8\") as ev_file:\n",
    "            ev_file.write(\"<unk>\\n\")\n",
    "            ev_file.write(\"</S>\\n\")\n",
    "            ev_file.write(\"</l>\\n\")\n",
    "            ev_file.write(\"<l>\\n\")\n",
    "            for line in file:\n",
    "                embed = [line[:-1]]\n",
    "                try:\n",
    "                    for num in w2v_model[line[:-1]]:\n",
    "                        embed.append(str(num))\n",
    "                    embed.append(\"\\n\")\n",
    "                    embed_text = \" \".join(embed)\n",
    "                    e_file.write(embed_text)\n",
    "                    ev_file.write(embed[0] + \"\\n\")\n",
    "                except KeyError:\n",
    "                    out_of_vocab.append(line[:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
