{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test script for LSTM models\n",
    "\n",
    "Test script for LSTM models. The model parameters must be the same as those used when training the model. \"restore_path\" is the path to the model to be tested. Test results are saved as a pickle file for detailed analysis.\n",
    "\n",
    "Testing does not take long on the legislative corpus: a few minutes on an intel i5 processor / 4GB memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.contrib.layers import xavier_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "data_path = \"\" # Data path\n",
    "\n",
    "text_data = [data_path + \"/leg_test_text.txt\"]  # Sentence training data (spacy parsed)\n",
    "text_whole_data = [data_path + \"/leg_test_original.txt\"] # Whole sentence (not tokenised)\n",
    "labels_data = [data_path + \"/leg_test_label.txt\"] # Labels for sentences\n",
    "embed_vocab_data = data_path + \"/leg_embeddings_vocab.txt\" # Embedding vocab: words from training sentences \n",
    "# for which embeddings exist and have been extracted in embed_file. (If full, this is \"embed_vocab.txt\")\n",
    "full_vocab_data = data_path + \"/total_vocab.txt\" # Full sentence vocab. (\"total_vocab.txt\")\n",
    "\n",
    "\n",
    "txt_eos = \"</S>\" # Special characters\n",
    "lbl_sos = \"<l>\"\n",
    "lbl_eos = \"</l>\"\n",
    "embed_file = data_path + \"/leg_embeddings.txt\" # Embeddings file (full is \"embeddings.txt\")\n",
    "\n",
    "restore_path = \"\" # Path to model to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters. These should match the parameters of the training model\n",
    "\n",
    "bi_directional = False\n",
    "num_dec_layers = 3\n",
    "num_bi_layers = 3\n",
    "\n",
    "batch_size = 25\n",
    "num_enc_units = 128\n",
    "num_dec_units = 128 # If uni-directional, then same as enc_units. If bi, make twice as big.\n",
    "\n",
    "beam_search = False\n",
    "beam_width = 4 # There are only 3 outputs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tf dataset: an iterator that returns batched data for processing.\n",
    "\n",
    "def build_dataset(text_data, text_whole_data, labels_data, embed_vocab_data, full_vocab_data, \n",
    "                  txt_eos, lbl_sos, lbl_eos, batch_size):\n",
    "\n",
    "    # Build the word to id lookup table from the text data. OOV words point at 0 = <unk> = random (but all same)\n",
    "    vocab_table = lookup_ops.index_table_from_file(embed_vocab_data, default_value=0)\n",
    "    \n",
    "    # Build a residual lookup table for all vocab, so can convert words back at end of process (evaluation only)\n",
    "    full_vocab_table = lookup_ops.index_table_from_file(full_vocab_data, default_value=0)\n",
    "\n",
    "    txt_eos_id = tf.cast(vocab_table.lookup(tf.constant(txt_eos)), tf.int32)\n",
    "    txt_full_eos_id = tf.cast(full_vocab_table.lookup(tf.constant(txt_eos)), tf.int32) # Probably not strictly necessary, since\n",
    "    # eos ends up in the same place in both vocab files.\n",
    "    lbl_sos_id = tf.cast(vocab_table.lookup(tf.constant(lbl_sos)), tf.int32)\n",
    "    lbl_eos_id = tf.cast(vocab_table.lookup(tf.constant(lbl_eos)), tf.int32)\n",
    "    fill = tf.constant(\"f\")\n",
    "\n",
    "    # Read each line of the text file. Each line is a sentence (where text has been tokenised using spacy)\n",
    "    # NB can pass multiple files to TextLineDataset (so can prep data in batches)\n",
    "    sent_data = tf.data.TextLineDataset(text_data)\n",
    "    sent_whole_data = tf.data.TextLineDataset(text_whole_data)\n",
    "    labels_data = tf.data.TextLineDataset(labels_data)\n",
    "\n",
    "    # For each line, split on white space\n",
    "    sent_data = sent_data.map(lambda string: tf.string_split([string]).values)\n",
    "    labels_data = labels_data.map(lambda label: tf.string_split([label]).values)\n",
    "    labels_data = labels_data.map(lambda label: tf.string_to_number(label, tf.int32))\n",
    "\n",
    "    # Lookup word ids (in the embedding vocab and in the full vocab)\n",
    "    embed_sent_data = sent_data.map(lambda token: tf.cast(vocab_table.lookup(token), tf.int32))\n",
    "    full_sent_data = sent_data.map(lambda token: tf.cast(full_vocab_table.lookup(token), tf.int32))\n",
    "\n",
    "    # Zip datasets together\n",
    "    sent_label_data = tf.data.Dataset.zip((full_sent_data, sent_whole_data, embed_sent_data, labels_data))\n",
    "    \n",
    "    # Create input dataset (labels prefixed by sos) and target dataset (labels suffixed with eos)\n",
    "    \n",
    "    sent_label_data = sent_label_data.map(lambda full_words, full_sent, embed_words, \n",
    "                                          labels: (full_words, full_sent, embed_words,\n",
    "                                                   tf.concat(([lbl_sos_id], labels), 0),\n",
    "                                                   tf.concat((labels, [lbl_eos_id]), 0),))\n",
    "\n",
    "    # Add seqeunce length\n",
    "    sent_label_data = sent_label_data.map(lambda full_words, full_sent, embed_words, \n",
    "                                          labels_in, labels_out: (full_words, full_sent, embed_words,\n",
    "                                                                  tf.size(embed_words),\n",
    "                                                                  tf.size(labels_in),\n",
    "                                                                  labels_in, \n",
    "                                                                  labels_out))\n",
    "    \n",
    "    # Random shuffle\n",
    "    # sent_label_data = sent_label_data.shuffle(buffer_size=5000)\n",
    "\n",
    "    # Batching the input, padding to the length of the longest sequence in the input. Can also bucket these. Form of dataset\n",
    "    # is: txt_ids_for_full_vocab, txt_ids_for_embed_vocab, text_size, label_size, labels_in, labels_out.\n",
    "    \n",
    "    batch_size = tf.constant(batch_size, tf.int64)\n",
    "    \n",
    "    batched_input = sent_label_data.padded_batch(batch_size, padded_shapes=(tf.TensorShape([None]),\n",
    "                                                                            tf.TensorShape([]),\n",
    "                                                                            tf.TensorShape([None]),\n",
    "                                                                            tf.TensorShape([]), \n",
    "                                                                            tf.TensorShape([]),\n",
    "                                                                            tf.TensorShape([None]), \n",
    "                                                                            tf.TensorShape([None])), \n",
    "                                                 padding_values=(txt_full_eos_id,\n",
    "                                                                 fill,\n",
    "                                                                 txt_eos_id,\n",
    "                                                                 0,\n",
    "                                                                 0,\n",
    "                                                                 lbl_eos_id, \n",
    "                                                                 lbl_eos_id))\n",
    "    iterator = batched_input.make_initializable_iterator()\n",
    "    return iterator, batch_size, lbl_sos_id, lbl_eos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparatory step to create_emb_matrix. Each line of the embedding file is a word followed by a space delimited numbers forming\n",
    "# the vector. load_embed_txt splits on white space and builds a dictionary where keys are the words in the embedding file\n",
    "\n",
    "def load_embed_txt(embed_file):\n",
    "    emb_dict = dict()\n",
    "    with codecs.getreader(\"utf-8\")(tf.gfile.GFile(embed_file, 'rb')) as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split(\" \")\n",
    "            word = tokens[0]\n",
    "            vec = list(map(float, tokens[1:]))\n",
    "            emb_dict[word] = vec\n",
    "            emb_size = len(vec)\n",
    "    return emb_dict, emb_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix (numpy array of embeddings). Includes an <unk> value for oov words. These are the values that are\n",
    "# looked-up when the model is run.\n",
    "\n",
    "def create_emb_matrix(embed_file):\n",
    "    emb_dict, emb_size = load_embed_txt(embed_file)\n",
    "    mat = np.array([emb_dict[token] for token in emb_dict.keys()])\n",
    "    emb_mat = tf.convert_to_tensor(mat, dtype=tf.float32)\n",
    "    return emb_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hack to help with the input to the decoder. Creates a matrix where keys and values are just integers in single item lists.\n",
    "\n",
    "def create_dec_matrix(num):\n",
    "    dec_dict = {}\n",
    "    for i in range(num):\n",
    "        dec_dict[i] = [i]\n",
    "    mat = np.array([dec_dict[token] for token in dec_dict.keys()])\n",
    "    dec_mat = tf.convert_to_tensor(mat, dtype=tf.float32)\n",
    "    return dec_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the id to vocab dictionary (reverse of the vocab lookup). This is for the \"embed vocab\" (i.e. where lots of words are\n",
    "# still mapped to <unk>)). This assumes there is both: an \"embed vocab file\", a file of the vocab for which embeddings exist and\n",
    "# an embed file. Recall unk and special characters are included in the vocab file, so no need to manaully add to the dictionary.\n",
    "# The words are just set out on each line of the file, so \"strip\" / \"split\" is a bit overkill but works well enough.\n",
    "\n",
    "def ids_to_embed_vocab(embed_vocab_data):\n",
    "    embed_vocab_dict = {}\n",
    "    with codecs.getreader(\"utf-8\")(tf.gfile.GFile(embed_vocab_data, 'rb')) as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            tokens = line.strip().split(\" \")\n",
    "            word = tokens[0]\n",
    "            embed_vocab_dict[count] = word\n",
    "            count += 1\n",
    "    return embed_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the id to vocab dictionary (reverse of the vocab lookup). This is for the full vocab. This is a hack, not really\n",
    "# necessary for the model but allows you to read the outputs easier (otherwise you would be left with lots of \"unks\" in the\n",
    "# final output.) We don't compute with these ids, they are just preserved through the batch input so we know what words went in.\n",
    "\n",
    "def ids_to_full_vocab(full_vocab_data):\n",
    "    full_vocab_dict = {}\n",
    "    with codecs.getreader(\"utf-8\")(tf.gfile.GFile(full_vocab_data, 'rb')) as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            tokens = line.strip().split(\" \")\n",
    "            word = tokens[0]\n",
    "            full_vocab_dict[count] = word\n",
    "            count += 1\n",
    "    return full_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single LSTM cell instance\n",
    "\n",
    "def single_cell(num_units, name):\n",
    "    single_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, name=name)\n",
    "    return single_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer RNN definition. The \"direction\" argument is just to help with naming when using bi-directional model.\n",
    "\n",
    "def RNN_cell(num_layers, num_units, direction):\n",
    "    if num_layers == 1:\n",
    "        cell_name = direction + \"_LSTM_layer\"\n",
    "        rnn_cell = single_cell(num_units, cell_name)\n",
    "    else:\n",
    "        cell_list = []\n",
    "        for i in range(num_layers):\n",
    "            cell_name = direction + \"_LSTM_layer_\" + str(i)\n",
    "            cell = single_cell(num_units, cell_name)\n",
    "            cell_list.append(cell)\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "    return rnn_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose cells and get output\n",
    "\n",
    "def build_encoder(encoder_emb_inp, num_layers, num_units, txt_size, bi_directional):\n",
    "    if bi_directional == True:\n",
    "        fw_cell = RNN_cell(num_layers, num_units, \"enc_fw\")\n",
    "        bw_cell = RNN_cell(num_layers, num_units, \"enc_bw\")\n",
    "        outputs, (encoder_fw_state, encoder_bw_state) = tf.nn.bidirectional_dynamic_rnn(fw_cell,\n",
    "                                                                                              bw_cell,\n",
    "                                                                                              encoder_emb_inp,\n",
    "                                                                                              sequence_length=txt_size,\n",
    "                                                                                              time_major=False,\n",
    "                                                                                              dtype = tf.float32)\n",
    "        \n",
    "        encoder_outputs = tf.concat(outputs, 2)\n",
    "        \n",
    "        if isinstance(encoder_fw_state, tuple) and isinstance(encoder_fw_state[0], tf.contrib.rnn.LSTMStateTuple):  # MultiLstmCell\n",
    "            encoder_state = tuple(map(\n",
    "                lambda fw_state, bw_state: tf.contrib.rnn.LSTMStateTuple(\n",
    "                    c=tf.concat((fw_state.c, bw_state.c), 1,\n",
    "                                name=\"bidirectional_concat_c\"),\n",
    "                    h=tf.concat((fw_state.h, bw_state.h), 1,\n",
    "                                name=\"bidirectional_concat_h\")),\n",
    "                encoder_fw_state, encoder_bw_state))\n",
    "        else:\n",
    "            encoder_state = tf.concat(\n",
    "                (encoder_fw_state, encoder_bw_state), 1,\n",
    "                name=\"bidirectional_state_concat\")\n",
    "    if bi_directional == False:\n",
    "        encoder_cells = RNN_cell(num_layers, num_units, \"enc_fw\")\n",
    "        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cells, \n",
    "                                                           encoder_emb_inp, \n",
    "                                                           sequence_length=txt_size, \n",
    "                                                           time_major=False, \n",
    "                                                           dtype = tf.float32)\n",
    "    \n",
    "    return encoder_outputs, encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a basic decoder\n",
    "\n",
    "def build_basic_decoder(decoder_cells, encoder_state, dec_mat, lbl_sos_id, lbl_eos_id, output_layer):\n",
    "    #helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, labels_size, time_major=False)\n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_mat, \n",
    "                                                      tf.fill([batch_size], lbl_sos_id), \n",
    "                                                      lbl_eos_id)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cells, helper, encoder_state, output_layer)\n",
    "    return decoder    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a beam search decoder\n",
    "\n",
    "def build_BM_decoder(decoder_cells, encoder_state, dec_mat, beam_width, lbl_sos_id, lbl_eos_id, output_layer):\n",
    "    start_tokens = tf.fill([batch_size], lbl_sos_id)\n",
    "    end_token = lbl_eos_id\n",
    "    decoder_initial_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width)\n",
    "    decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=decoder_cells, embedding=dec_mat, \n",
    "                                                           start_tokens=start_tokens,\n",
    "                                                           end_token=end_token, \n",
    "                                                           initial_state=decoder_initial_state, \n",
    "                                                           beam_width=beam_width,\n",
    "                                                           output_layer=output_layer,\n",
    "                                                           length_penalty_weight=0.0)\n",
    "    return decoder    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, num_enc_units, num_dec_units, num_dec_layers, \n",
    "                 num_bi_layers, bi_directional, embed_words, full_words, text_whole_data, txt_size, labels_size,\n",
    "                 labels_in, labels_out, beam_search, beam_width, batch_size, lbl_sos_id, lbl_eos_id):\n",
    "        \n",
    "        self.num_enc_units = num_enc_units\n",
    "        \n",
    "        self.words_in = embed_words\n",
    "        self.full_words_in = full_words\n",
    "        self.text_whole_in = text_whole_data\n",
    "        \n",
    "        with tf.variable_scope(\"main\", initializer=xavier_initializer()):\n",
    "            \n",
    "            # Inputs\n",
    "            \n",
    "            mask_labels = tf.sequence_mask(labels_size, dtype=tf.int32) # To mask the padded input\n",
    "            labels_in = labels_in * mask_labels\n",
    "            \n",
    "            self.labels_out = labels_out\n",
    "            self.mask_labels = mask_labels\n",
    "            \n",
    "            encoder_emb_inp = tf.nn.embedding_lookup(emb_mat, embed_words) # Encoder embedding lookup\n",
    "            decoder_emb_inp = tf.nn.embedding_lookup(dec_mat, labels_in) # Decoder embedding lookup (easiest way to get it in\n",
    "            # right shape)\n",
    "            \n",
    "            # Encoder definition (by default, encoder_state is just the final state). Encoder can be multi-layers and\n",
    "            # bi-directional\n",
    "            \n",
    "            encoder_outputs, encoder_state = build_encoder(encoder_emb_inp, num_bi_layers, num_enc_units, \n",
    "                                                           txt_size, bi_directional)\n",
    "            \n",
    "            # Decoder definition. Number of decoder layers is the same as the number of encoder layers, but needed be. The \n",
    "            # helper is defined seperately and can be adjusted for greedy / beam decoding at inference.\n",
    "            \n",
    "            decoder_cells = RNN_cell(num_dec_layers, num_dec_units, \"dec\")\n",
    "            \n",
    "            # Output layer which takes decoder output and maps to 3 categories (0,1,2) - these are the same as the target labels.\n",
    "            # Recall 2 just maps to </l>, which is the prediction for </s>\n",
    "            \n",
    "            output_layer = layers_core.Dense(3, use_bias=False, name=\"output_projection\")\n",
    "            \n",
    "            if beam_search == False:\n",
    "                decoder = build_basic_decoder(decoder_cells, encoder_state, dec_mat, lbl_sos_id, lbl_eos_id, output_layer)\n",
    "            else:\n",
    "                decoder = build_BM_decoder(decoder_cells, encoder_state, dec_mat, beam_width, \n",
    "                                           lbl_sos_id, lbl_eos_id, output_layer)          \n",
    "            \n",
    "            outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder,\n",
    "                                                                                impute_finished = False,\n",
    "                                                                                output_time_major=False)\n",
    "            \n",
    "            # Decoder just runs until it gets to the end, but could impose a max length (e.g. length of labels)\n",
    "                       \n",
    "            self.preds = tf.argmax(outputs[0], axis=2) # Take the argmax to get the prediction. Distinct from beam / non-beam\n",
    "            # search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect outputs\n",
    "\n",
    "test_inputs = []\n",
    "test_inputs_whole = []\n",
    "predictions = []\n",
    "predicted_labels = []\n",
    "targets = []\n",
    "target_labels = []\n",
    "errors = []\n",
    "error_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the graph\n",
    "\n",
    "with tf.Graph().as_default(): \n",
    "\n",
    "    iterator, batch_size_, lbl_sos_id_, lbl_eos_id_ = build_dataset(text_data, text_whole_data, labels_data, \n",
    "                                                                    embed_vocab_data, full_vocab_data, txt_eos, \n",
    "                                                                    lbl_sos, lbl_eos, batch_size)\n",
    "    \n",
    "    emb_mat = create_emb_matrix(embed_file)\n",
    "    dec_mat = create_dec_matrix(4)\n",
    "    \n",
    "    random_embeddings = np.random.uniform(low=-1, high=1, size=(4,300)) # A random choice for unk and other special characters\n",
    "    embeddings = tf.Variable(tf.convert_to_tensor(random_embeddings, dtype=tf.float32), name=\"saved_embeddings\")\n",
    "    emb_mat = tf.concat((embeddings, emb_mat), 0)\n",
    "    \n",
    "    ids_to_embed_vocab_ = ids_to_embed_vocab(embed_vocab_data)\n",
    "    ids_to_full_vocab_ = ids_to_full_vocab(full_vocab_data)\n",
    "    \n",
    "    # A call to the iterator for inputs\n",
    "    \n",
    "    full_words_, text_whole_data_, embed_words_, txt_size_, label_size_, labels_in_, labels_out_  = iterator.get_next()\n",
    "    \n",
    "    # Model instantiation\n",
    "    \n",
    "    model = Model(num_enc_units, num_dec_units, num_dec_layers, num_bi_layers, bi_directional,\n",
    "                  embed_words_, full_words_, text_whole_data_, txt_size_, label_size_, labels_in_, labels_out_, \n",
    "                  beam_search, beam_width, batch_size_, lbl_sos_id_, lbl_eos_id_)\n",
    "    \n",
    "    # Initialise variables\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    saver = tf.train.Saver() # Save for variables. Not full graph.\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        saver.restore(sess, restore_path)\n",
    "        print(\"Model restored.\")\n",
    "        \n",
    "        # Initialise the vocab tables\n",
    "        sess.run(tf.tables_initializer())\n",
    "        sess.run(iterator.initializer)\n",
    "        counter = 0\n",
    "        \n",
    "        # Batch processing loop.\n",
    "        while True:\n",
    "            try:\n",
    "                preds, full_words_in, text_whole_in, labels_out, mask_labels = sess.run([model.preds, \n",
    "                                                                                         model.full_words_in, \n",
    "                                                                                         model.text_whole_in, \n",
    "                                                                                         model.labels_out, \n",
    "                                                                                         model.mask_labels])\n",
    "                for j in range(len(full_words_in)):\n",
    "\n",
    "                    full_sent = []\n",
    "                    target_sent = []\n",
    "                    predicted_sent = []\n",
    "                    de_bug = False\n",
    "                    \n",
    "                    target_labels.append(labels_out[j])\n",
    "                    predicted_labels.append(preds[j])\n",
    "\n",
    "                    for i in range(len(full_words_in[j])):\n",
    "                        if mask_labels[j][i] == 1:\n",
    "                            full_sent.append(ids_to_full_vocab_[full_words_in[j][i]])\n",
    "                            try:\n",
    "                                if preds[j][i] != 0:\n",
    "                                    predicted_sent.append(ids_to_full_vocab_[full_words_in[j][i]])\n",
    "                            except IndexError:\n",
    "                                de_bug = True\n",
    "                            try:\n",
    "                                if labels_out[j][i] != 0:\n",
    "                                    target_sent.append(ids_to_full_vocab_[full_words_in[j][i]])\n",
    "                            except IndexError:\n",
    "                                de_bug = True\n",
    "\n",
    "                    if de_bug == False:\n",
    "                        test_inputs.append(full_sent)\n",
    "                        test_inputs_whole.append(text_whole_in[j])\n",
    "                        predictions.append(predicted_sent)\n",
    "                        targets.append(target_sent)\n",
    "                    else:\n",
    "                        test_inputs.append(full_sent)\n",
    "                        predictions.append([\"de_bug_error\"])\n",
    "                        targets.append([\"de_bug_error\"])\n",
    "                        errors.append([full_sent, preds[j], labels_out[j]])\n",
    "                        error_count += 1\n",
    "\n",
    "                    counter += 1\n",
    "                    if counter % 1000 == 0:\n",
    "                        print(\"Number of examples processed:\")\n",
    "                        print(counter)\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        print(\"End of input file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the test data\n",
    "\n",
    "import pickle\n",
    "\n",
    "test_outputs = {}\n",
    "test_outputs['stringInputs'] = test_inputs\n",
    "test_outputs['wholeInput'] = test_inputs_whole\n",
    "test_outputs['predictions'] = predictions\n",
    "test_outputs['predicted_labels'] = predicted_labels\n",
    "test_outputs['targets'] = targets\n",
    "test_outputs['target_labels'] = target_labels\n",
    "\n",
    "results_file = \"\" # Set output file\n",
    "\n",
    "with open(results_file, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(test_outputs, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
