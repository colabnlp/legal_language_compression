{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning with ULMFiT script\n",
    "\n",
    "This script implements transfer learning for the LSTM models using the ULMFiT procedures. The core parameters are specified in \"slanted_learning_rate\". The rigidity of the tensorflow computational graph means that the procedures are hard coded (with separate learning rates and optimisers specified for each model layer). This is hugely inefficient and it might be that a cleaner implementation is possible in newer versions of tensorflow.\n",
    "\n",
    "Given the size of the legislative training dataset, these efficiencies don't much matter: one training epoch takes around 80 seconds on an intel i5 processor / 4GB memory.\n",
    "\n",
    "Adapted from <https://github.com/tensorflow/nmt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.contrib.layers import xavier_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "data_path = \"\" # Data path\n",
    "\n",
    "text_data = [data_path + \"/leg_train_text.txt\"]  # Sentence training data (spacy parsed)\n",
    "text_whole_data = [data_path + \"/leg_train_original.txt\"] # Whole sentence (not tokenised)\n",
    "labels_data = [data_path + \"/leg_train_label.txt\"] # Labels for sentences\n",
    "embed_vocab_data = data_path + \"/leg_embeddings_vocab.txt\" # Embedding vocab: words from training sentences \n",
    "# for which embeddings exist and have been extracted in embed_file. (If full, this is \"embed_vocab.txt\")\n",
    "full_vocab_data = data_path + \"/total_vocab.txt\" # Full sentence vocab. (\"total_vocab.txt\")\n",
    "\n",
    "\n",
    "txt_eos = \"</S>\" # Special characters\n",
    "lbl_sos = \"<l>\"\n",
    "lbl_eos = \"</l>\"\n",
    "embed_file = data_path + \"/leg_embeddings.txt\" # Embeddings file (full is \"embeddings.txt\")\n",
    "\n",
    "restore_path = \"./LSTM_base/model.ckpt\"\n",
    "save_model = True # Set to True if you want to save model variables\n",
    "log_path = \"\" # Log directory\n",
    "save_path = \"\" # Save model path, only used if save_path is True\n",
    "log_freq = 100 # Show some outputs every log_freq training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "\n",
    "num_layers = 3\n",
    "num_total_layers = 7\n",
    "\n",
    "num_units = 128 # If uni-directional, then same as enc_units. If bi, make twice as big.\n",
    "\n",
    "beam_search = False\n",
    "beam_width = 4 # There are only 3 outputs...\n",
    "\n",
    "batch_size = 25\n",
    "forget_bias = 0\n",
    "dropout = 0.2\n",
    "max_gradient_norm = 1\n",
    "learning_rate = 0.002 # This doesn't do anything - see slanted_learning_rate\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tf dataset: an iterator that returns batched data for training.\n",
    "\n",
    "def build_dataset(text_data, labels_data, embed_vocab_data, full_vocab_data, txt_eos, lbl_sos, lbl_eos, batch_size):\n",
    "\n",
    "    # Build the word to id lookup table from the text data. OOV words point at 0 = <unk> = random (but all same)\n",
    "    vocab_table = lookup_ops.index_table_from_file(embed_vocab_data, default_value=0)\n",
    "    \n",
    "    # Build a residual lookup table for all vocab, so can convert words back at end of process (evaluation only)\n",
    "    full_vocab_table = lookup_ops.index_table_from_file(full_vocab_data, default_value=0)\n",
    "\n",
    "    txt_eos_id = tf.cast(vocab_table.lookup(tf.constant(txt_eos)), tf.int32)\n",
    "    txt_full_eos_id = tf.cast(full_vocab_table.lookup(tf.constant(txt_eos)), tf.int32) # Probably not strictly necessary, since\n",
    "    # eos ends up in the same place in both vocab files.\n",
    "    lbl_sos_id = tf.cast(vocab_table.lookup(tf.constant(lbl_sos)), tf.int32)\n",
    "    lbl_eos_id = tf.cast(vocab_table.lookup(tf.constant(lbl_eos)), tf.int32)\n",
    "\n",
    "    # Read each line of the text file. Each line is a sentence (where text has been tokenised using spacy)\n",
    "    # NB can pass multiple files to TextLineDataset (so can prep data in batches)\n",
    "    sent_data = tf.data.TextLineDataset(text_data)\n",
    "    labels_data = tf.data.TextLineDataset(labels_data)\n",
    "\n",
    "    # For each line, split on white space\n",
    "    sent_data = sent_data.map(lambda string: tf.string_split([string]).values)\n",
    "    labels_data = labels_data.map(lambda label: tf.string_split([label]).values)\n",
    "    labels_data = labels_data.map(lambda label: tf.string_to_number(label, tf.int32))\n",
    "\n",
    "    # Lookup word ids (in the embedding vocab and in the full vocab)\n",
    "    embed_sent_data = sent_data.map(lambda token: tf.cast(vocab_table.lookup(token), tf.int32))\n",
    "    full_sent_data = sent_data.map(lambda token: tf.cast(full_vocab_table.lookup(token), tf.int32))\n",
    "\n",
    "    # Zip datasets together\n",
    "    sent_label_data = tf.data.Dataset.zip((full_sent_data, embed_sent_data, labels_data))\n",
    "    \n",
    "    # Create input dataset (labels prefixed by sos) and target dataset (labels suffixed with eos)\n",
    "    \n",
    "    sent_label_data = sent_label_data.map(lambda full_words, embed_words, labels: (full_words, embed_words,\n",
    "                                                                                  tf.concat(([lbl_sos_id], labels), 0),\n",
    "                                                                                  tf.concat((labels, [lbl_eos_id]), 0),))\n",
    "\n",
    "    # Add seqeunce length\n",
    "    sent_label_data = sent_label_data.map(lambda full_words, embed_words, labels_in, labels_out: (full_words, embed_words,\n",
    "                                                                                                  tf.size(embed_words),\n",
    "                                                                                                  tf.size(labels_in),\n",
    "                                                                                                  labels_in, \n",
    "                                                                                                  labels_out))\n",
    "    \n",
    "    # Random shuffle\n",
    "    sent_label_data = sent_label_data.shuffle(buffer_size=5000)\n",
    "\n",
    "    # Batching the input, padding to the length of the longest sequence in the input. Can also bucket these. Form of dataset\n",
    "    # is: txt_ids_for_full_vocab, txt_ids_for_embed_vocab, text_size, label_size, labels_in, labels_out.\n",
    "    \n",
    "    batch_size = tf.constant(batch_size, tf.int64)\n",
    "    \n",
    "    batched_input = sent_label_data.padded_batch(batch_size, padded_shapes=(tf.TensorShape([None]),\n",
    "                                                                            tf.TensorShape([None]),\n",
    "                                                                            tf.TensorShape([]), \n",
    "                                                                            tf.TensorShape([]),\n",
    "                                                                            tf.TensorShape([None]), \n",
    "                                                                            tf.TensorShape([None])), \n",
    "                                                 padding_values=(txt_full_eos_id,\n",
    "                                                                 txt_eos_id,\n",
    "                                                                 0,\n",
    "                                                                 0,\n",
    "                                                                 lbl_eos_id, \n",
    "                                                                 lbl_eos_id))\n",
    "    iterator = batched_input.make_initializable_iterator()\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparatory step to create_emb_matrix. Each line of the embedding file is a word followed by a space delimited numbers forming\n",
    "# the vector. load_embed_txt splits on white space and builds a dictionary where keys are the words in the embedding file\n",
    "\n",
    "def load_embed_txt(embed_file):\n",
    "    emb_dict = dict()\n",
    "    with codecs.getreader(\"utf-8\")(tf.gfile.GFile(embed_file, 'rb')) as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split(\" \")\n",
    "            word = tokens[0]\n",
    "            vec = list(map(float, tokens[1:]))\n",
    "            emb_dict[word] = vec\n",
    "            emb_size = len(vec)\n",
    "    return emb_dict, emb_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix (numpy array of embeddings). Includes an <unk> value for oov words. These are the values that are\n",
    "# looked-up when the model is run.\n",
    "\n",
    "def create_emb_matrix(embed_file):\n",
    "    emb_dict, emb_size = load_embed_txt(embed_file)\n",
    "    mat = np.array([emb_dict[token] for token in emb_dict.keys()])\n",
    "    emb_mat = tf.convert_to_tensor(mat, dtype=tf.float32)\n",
    "    return emb_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hack to help with the input to the decoder. Creates a matrix where keys and values are just integers in single item lists.\n",
    "\n",
    "def create_dec_matrix(num):\n",
    "    dec_dict = {}\n",
    "    for i in range(num):\n",
    "        dec_dict[i] = [i]\n",
    "    mat = np.array([dec_dict[token] for token in dec_dict.keys()])\n",
    "    dec_mat = tf.convert_to_tensor(mat, dtype=tf.float32)\n",
    "    return dec_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the id to vocab dictionary (reverse of the vocab lookup). This is for the \"embed vocab\" (i.e. where lots of words are\n",
    "# still mapped to <unk>)). This assumes there is both: an \"embed vocab file\", a file of the vocab for which embeddings exist and\n",
    "# an embed file. Recall unk and special characters are included in the vocab file, so no need to manaully add to the dictionary.\n",
    "# The words are just set out on each line of the file, so \"strip\" / \"split\" is a bit overkill but works well enough.\n",
    "\n",
    "def ids_to_embed_vocab(embed_vocab_data):\n",
    "    embed_vocab_dict = {}\n",
    "    with codecs.getreader(\"utf-8\")(tf.gfile.GFile(embed_vocab_data, 'rb')) as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            tokens = line.strip().split(\" \")\n",
    "            word = tokens[0]\n",
    "            embed_vocab_dict[count] = word\n",
    "            count += 1\n",
    "    return embed_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the id to vocab dictionary (reverse of the vocab lookup). This is for the full vocab. This is a hack, not really\n",
    "# necessary for the model but allows you to read the outputs easier (otherwise you would be left with lots of \"unks\" in the\n",
    "# final output.) We don't compute with these ids, they are just preserved through the batch input so we know what words went in.\n",
    "\n",
    "def ids_to_full_vocab(full_vocab_data):\n",
    "    full_vocab_dict = {}\n",
    "    with codecs.getreader(\"utf-8\")(tf.gfile.GFile(full_vocab_data, 'rb')) as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            tokens = line.strip().split(\" \")\n",
    "            word = tokens[0]\n",
    "            full_vocab_dict[count] = word\n",
    "            count += 1\n",
    "    return full_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single LSTM cell instance with dropout option.\n",
    "\n",
    "def single_cell(num_units, forget_bias, dropout, name):\n",
    "    single_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, forget_bias=forget_bias, name=name)\n",
    "    if dropout > 0.0:\n",
    "        single_cell = tf.nn.rnn_cell.DropoutWrapper(cell=single_cell, input_keep_prob=(1.0 - dropout))\n",
    "    return single_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer RNN definition. The \"direction\" argument is just to help with naming when using bi-directional model.\n",
    "\n",
    "def RNN_cell(num_layers, num_units, forget_bias, dropout, direction):\n",
    "    if num_layers == 1:\n",
    "        cell_name = direction + \"_LSTM_layer\"\n",
    "        rnn_cell = single_cell(num_units, forget_bias, dropout, cell_name)\n",
    "    else:\n",
    "        cell_list = []\n",
    "        for i in range(num_layers):\n",
    "            cell_name = direction + \"_LSTM_layer_\" + str(i)\n",
    "            cell = single_cell(num_units, forget_bias, dropout, cell_name)\n",
    "            cell_list.append(cell)\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "    return rnn_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build bi-directional LSTM (just add direction name prefixes)\n",
    "\n",
    "def build_bi_directional_LSTM(num_units, forget_bias, dropout, num_layers):\n",
    "    \n",
    "    fw_cell = RNN_cell(num_layers, num_units, forget_bias, dropout, \"fw\")\n",
    "    bw_cell = RNN_cell(num_layers, num_units, forget_bias, dropout, \"bw\")\n",
    "    \n",
    "    return fw_cell, bw_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the slanted learning rates to use at each training iteration. Returns a list of learning rates, one per training\n",
    "# iteration. Note params are hard coded (see in particular the number of training examples).\n",
    "\n",
    "def slanted_learning_rate(epochs, num_total_layers, batch_size):\n",
    "    LRmax = 0.01\n",
    "    ratio = 32\n",
    "    T = (750*epochs)/batch_size # Number of training examples.\n",
    "    cut_frac = 0.1\n",
    "    cut = T * cut_frac\n",
    "    reduce_factor = 1/2.6\n",
    "\n",
    "    iterations = np.arange(int(T))\n",
    "    \n",
    "    s_l_r = np.zeros((num_total_layers, int(T)))\n",
    "    \n",
    "    for l in range(num_total_layers):                  \n",
    "        for i in iterations:\n",
    "            if i < cut:\n",
    "                p = i/cut\n",
    "                LR_i = LRmax*((1+p*(ratio-1))/ratio)\n",
    "            else:\n",
    "                p = 1 - ((i-cut)/(cut*(1/cut_frac-1)))\n",
    "                LR_i = LRmax*((1+p*(ratio-1))/ratio)\n",
    "            if i > l*(T/epochs):\n",
    "                s_l_r[l][i]= LR_i*(reduce_factor**l)\n",
    "    \n",
    "    t_list = []\n",
    "    for l in range(num_total_layers):\n",
    "        s_l_r_t = tf.convert_to_tensor(s_l_r[l], dtype=tf.float32)\n",
    "        t_list.append([s_l_r_t])\n",
    "    s_l_r_t = tf.concat(t_list,0)\n",
    "    return s_l_r_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, dropout, num_units, num_layers, forget_bias, \n",
    "                 embed_words, full_words, txt_size, labels_size, labels_in, labels_out,\n",
    "                 global_step):\n",
    "        \n",
    "        self.global_step = global_step\n",
    "        self.learning_rates_ = slanted_learning_rate(epochs, num_total_layers, batch_size)\n",
    "        \n",
    "        self.learning_rate1 = self.learning_rates_[0][global_step]\n",
    "        self.learning_rate2 = self.learning_rates_[1][global_step]\n",
    "        self.learning_rate3 = self.learning_rates_[2][global_step]\n",
    "        self.learning_rate4 = self.learning_rates_[3][global_step]\n",
    "        self.learning_rate5 = self.learning_rates_[4][global_step]\n",
    "        self.learning_rate6 = self.learning_rates_[5][global_step]\n",
    "        self.learning_rate7 = self.learning_rates_[6][global_step]\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.num_units = num_units\n",
    "        self.forget_bias = forget_bias\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.words_in = embed_words\n",
    "        self.full_words_in = full_words\n",
    "        \n",
    "        with tf.variable_scope(\"main\", initializer=xavier_initializer()):\n",
    "            \n",
    "            # Inputs\n",
    "            \n",
    "            mask_labels = tf.sequence_mask(labels_size, dtype=tf.int32) # To mask the padded input\n",
    "            labels_in = labels_in * mask_labels\n",
    "            \n",
    "            self.labels_out = labels_out\n",
    "            self.mask_labels = mask_labels\n",
    "            \n",
    "            encoder_emb_inp = tf.nn.embedding_lookup(emb_mat, embed_words) # Encoder embedding lookup\n",
    "            decoder_emb_inp = tf.nn.embedding_lookup(dec_mat, labels_in) # Decoder embedding lookup (easiest way to get it in\n",
    "            # right shape)\n",
    "            \n",
    "            # Encoder definition (by default, encoder_state is just the final state). Encoder can be multi-layers and\n",
    "            # bi-directional\n",
    "            \n",
    "            encoder_cells = RNN_cell(num_layers, num_units, forget_bias, dropout, \"enc_fw\")\n",
    "            \n",
    "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cells, \n",
    "                                                               encoder_emb_inp, \n",
    "                                                               sequence_length=txt_size, \n",
    "                                                               time_major=False, \n",
    "                                                               dtype = tf.float32)\n",
    "            \n",
    "            # Decoder definition. Number of decoder layers is the same as the number of encoder layers, but needed be. The \n",
    "            # helper is defined seperately and can be adjusted for greedy / beam decoding at inference.\n",
    "            \n",
    "            decoder_cells = RNN_cell(num_layers, num_units, forget_bias, dropout, \"dec\")\n",
    "            \n",
    "            helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, \n",
    "                                                       labels_size, \n",
    "                                                       time_major=False)\n",
    "            \n",
    "            # Output layer which takes decoder output and maps to 3 categories (0,1,2) - these are the same as the target labels.\n",
    "            # Recall 2 just maps to </l>, which is the prediction for </s>\n",
    "            \n",
    "            output_layer = layers_core.Dense(3, use_bias=False, name=\"output_projection\")\n",
    "            \n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cells, helper, encoder_state, output_layer)\n",
    "            \n",
    "            outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder,\n",
    "                                                                                output_time_major=False)\n",
    "            \n",
    "            # Decoder just runs until it gets to the end, but could impose a max length (e.g. length of labels)\n",
    "            \n",
    "            # Calculate loss: By logits we just mean the outputs of the decoder (after output_layer). crossent takes normalised\n",
    "            # output probability prediction for each class (i.e. the softmax of the logits) and takes cross-entropy with the \n",
    "            # actual labels.\n",
    "            \n",
    "            self.logits = outputs[0]\n",
    "            crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels_out, logits=self.logits)\n",
    "            self.loss = tf.reduce_sum(crossent * tf.cast(mask_labels, tf.float32)) / tf.cast(batch_size, tf.float32)\n",
    "            \n",
    "            ########################################################################################################\n",
    "            # Transfer learning regime...\n",
    "            ########################################################################################################\n",
    "            \n",
    "            epoch_count = tf.floor_div(global_step*batch_size, 750)+1\n",
    "            \n",
    "            opt1 = tf.train.GradientDescentOptimizer(self.learning_rates_[0][global_step])\n",
    "            opt2 = tf.train.GradientDescentOptimizer(self.learning_rates_[1][global_step])\n",
    "            opt3 = tf.train.GradientDescentOptimizer(self.learning_rates_[2][global_step])\n",
    "            opt4 = tf.train.GradientDescentOptimizer(self.learning_rates_[3][global_step])\n",
    "            opt5 = tf.train.GradientDescentOptimizer(self.learning_rates_[4][global_step])\n",
    "            opt6 = tf.train.GradientDescentOptimizer(self.learning_rates_[5][global_step])\n",
    "            opt7 = tf.train.GradientDescentOptimizer(self.learning_rates_[6][global_step])\n",
    "            \n",
    "            t_variables1 = tf.trainable_variables(scope=\"main/decoder/output_projection/\")\n",
    "            t_variables2 = tf.trainable_variables(scope=\"main/decoder/multi_rnn_cell/cell_2/dec_LSTM_layer_2/\")\n",
    "            t_variables3 = tf.trainable_variables(scope=\"main/decoder/multi_rnn_cell/cell_1/dec_LSTM_layer_1/\")\n",
    "            t_variables4 = tf.trainable_variables(scope=\"main/decoder/multi_rnn_cell/cell_0/dec_LSTM_layer_0/\")\n",
    "            t_variables5 = tf.trainable_variables(scope=\"main/rnn/multi_rnn_cell/cell_2/enc_fw_LSTM_layer_2/\")\n",
    "            t_variables6 = tf.trainable_variables(scope=\"main/rnn/multi_rnn_cell/cell_1/enc_fw_LSTM_layer_1/\")\n",
    "            t_variables7 = tf.trainable_variables(scope=\"main/rnn/multi_rnn_cell/cell_0/enc_fw_LSTM_layer_0/\")\n",
    "            \n",
    "            gradients1, variables1 = zip(*opt1.compute_gradients(self.loss, var_list=t_variables1))\n",
    "            gradients2, variables2 = zip(*opt2.compute_gradients(self.loss, var_list=t_variables2))\n",
    "            gradients3, variables3 = zip(*opt3.compute_gradients(self.loss, var_list=t_variables3))\n",
    "            gradients4, variables4 = zip(*opt4.compute_gradients(self.loss, var_list=t_variables4))\n",
    "            gradients5, variables5 = zip(*opt5.compute_gradients(self.loss, var_list=t_variables5))\n",
    "            gradients6, variables6 = zip(*opt6.compute_gradients(self.loss, var_list=t_variables6))\n",
    "            gradients7, variables7 = zip(*opt7.compute_gradients(self.loss, var_list=t_variables7))\n",
    "            \n",
    "            train_opt1 = opt1.apply_gradients(zip(gradients1, variables1), global_step=global_step)\n",
    "            train_opt2 = opt2.apply_gradients(zip(gradients2, variables2))\n",
    "            train_opt3 = opt3.apply_gradients(zip(gradients3, variables3))\n",
    "            train_opt4 = opt4.apply_gradients(zip(gradients4, variables4))\n",
    "            train_opt5 = opt5.apply_gradients(zip(gradients5, variables5))\n",
    "            train_opt6 = opt6.apply_gradients(zip(gradients6, variables6))\n",
    "            train_opt7 = opt7.apply_gradients(zip(gradients7, variables7))\n",
    "            \n",
    "            grad_opts = [train_opt1, train_opt2, train_opt3, train_opt4, train_opt5, train_opt6, train_opt7]\n",
    "            \n",
    "            self.train_ops = tf.group(grad_opts)\n",
    "            \n",
    "            self.preds = tf.argmax(self.logits, axis=2)\n",
    "            \n",
    "            # Summaries: Tensorflow summaries\n",
    "            \n",
    "            self.make_summaries(self.learning_rate1, self.learning_rate2, self.learning_rate3, \n",
    "                                self.learning_rate4, self.learning_rate5, self.learning_rate6,\n",
    "                                self.learning_rate7, self.loss)\n",
    "                        \n",
    "    def make_summaries(self, learning_rate1, learning_rate2, learning_rate3, learning_rate4, \n",
    "                       learning_rate5, learning_rate6, learning_rate7, loss):\n",
    "        \n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "        tf.summary.scalar(\"learning_rate_dense_layer\", learning_rate1)\n",
    "        tf.summary.scalar(\"learning_dec_layer2\", learning_rate2)\n",
    "        tf.summary.scalar(\"learning_dec_layer1\", learning_rate3)\n",
    "        tf.summary.scalar(\"learning_dec_layer0\", learning_rate4)\n",
    "        tf.summary.scalar(\"learning_enc_layer2\", learning_rate5)\n",
    "        tf.summary.scalar(\"learning_enc_layer1\", learning_rate6)\n",
    "        tf.summary.scalar(\"learning_enc_layer0\", learning_rate7)\n",
    "        \n",
    "        self.merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the graph\n",
    "\n",
    "with tf.Graph().as_default(): \n",
    "\n",
    "    iterator = build_dataset(text_data, labels_data, embed_vocab_data, full_vocab_data, txt_eos, lbl_sos, lbl_eos, batch_size)\n",
    "    \n",
    "    emb_mat = create_emb_matrix(embed_file)\n",
    "    dec_mat = create_dec_matrix(4)\n",
    "    \n",
    "    random_embeddings = np.random.uniform(low=-1, high=1, size=(4,300)) # A random choice for unk and other special characters\n",
    "    embeddings = tf.Variable(tf.convert_to_tensor(random_embeddings, dtype=tf.float32), name=\"saved_embeddings\")\n",
    "    emb_mat = tf.concat((embeddings, emb_mat), 0)\n",
    "    \n",
    "    ids_to_embed_vocab = ids_to_embed_vocab(embed_vocab_data)\n",
    "    ids_to_full_vocab = ids_to_full_vocab(full_vocab_data)\n",
    "    \n",
    "    # A call to the iterator for inputs\n",
    "    \n",
    "    full_words_, embed_words_, txt_size_, label_size_, labels_in_, labels_out_ = iterator.get_next()\n",
    "    \n",
    "    # Model instantiation\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "    model = Model(dropout, num_units, num_layers, \n",
    "                  forget_bias, embed_words_, full_words_, \n",
    "                  txt_size_, label_size_, labels_in_, labels_out_, global_step)\n",
    "    \n",
    "    # Initialise variables\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    t_variables = tf.trainable_variables()\n",
    "    saver = tf.train.Saver(var_list=t_variables) # Saver for variables. Not full graph.\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(log_path, sess.graph)\n",
    "                \n",
    "        # Restore variables if present.\n",
    "        if restore_path == None:\n",
    "            sess.run(init)\n",
    "        else:\n",
    "            global_step.initializer.run()\n",
    "            saver.restore(sess, restore_path)\n",
    "            print(\"Model restored.\")\n",
    "        \n",
    "        # Initialise the vocab tables\n",
    "        sess.run(tf.tables_initializer())\n",
    "        counter = 0\n",
    "        # Training loop.\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            epoch_start = time.time()\n",
    "            sess.run(iterator.initializer)\n",
    "            while True:\n",
    "                try:\n",
    "                    _, summary, loss = sess.run([model.train_ops, model.merged, model.loss])\n",
    "                    \n",
    "                    train_writer.add_summary(summary, counter)\n",
    "                    train_writer.flush()\n",
    "                    losses.append(loss) # Counter for epoch loss\n",
    "                    counter += 1\n",
    "                    \n",
    "                    #print(sess.run(global_step))\n",
    "                    \n",
    "                    if counter % log_freq == 0:\n",
    "                        \n",
    "                        # Get the values from model\n",
    "                        preds, full_words_in, labels_out, mask_labels = sess.run([model.preds,\n",
    "                                                                                  model.full_words_in, \n",
    "                                                                                  model.labels_out, \n",
    "                                                                                  model.mask_labels])\n",
    "\n",
    "                        # pick one of the entries in the current batch\n",
    "                        j = np.random.randint(0, batch_size)\n",
    "                            \n",
    "                        full_sent = []\n",
    "                        target_sent = []\n",
    "                        predicted_sent = []\n",
    "\n",
    "                        for i in range(len(full_words_in[j])):\n",
    "                            if mask_labels[j][i] == 1:\n",
    "                                full_sent.append(ids_to_full_vocab[full_words_in[j][i]])\n",
    "                                if preds[j][i] != 0:\n",
    "                                    predicted_sent.append(ids_to_full_vocab[full_words_in[j][i]])\n",
    "                                if labels_out[j][i] != 0:\n",
    "                                    target_sent.append(ids_to_full_vocab[full_words_in[j][i]])\n",
    "\n",
    "                        print(\"Input sentence is:\")\n",
    "                        print(\" \".join(full_sent))\n",
    "                        print(\"Target sentence is:\")\n",
    "                        print(\" \".join(target_sent))\n",
    "                        print(\"Predicted sentence is:\")\n",
    "                        print(\" \".join(predicted_sent))\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    \n",
    "                    average_loss = sum(losses) / len(losses)\n",
    "                    elapsed_time = (time.time() - epoch_start)\n",
    "                    print(\"Epoch run time: %s\" % elapsed_time)\n",
    "                    print(\"Average epoch loss: %s\" % average_loss)\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "        if save_model == True:\n",
    "            saver.save(sess, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
