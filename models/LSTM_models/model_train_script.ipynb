{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training script for LSTM models\n",
    "\n",
    "This model was build using tensorflow v1.10.0 and has not been tested on more recent versions.\n",
    "\n",
    "Training on the legislative corpus does not require a hardware accelerator. One training epoch on the legislative data (with batch size 25) takes about a minute on an intel i5 processor / 4GB memory. Training on the news headline corpus takes significantly longer: about an hour per epoch on a Google Colab GPU.\n",
    "\n",
    "Adapted from <https://github.com/tensorflow/nmt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "from tensorflow.contrib.layers import xavier_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "data_path = \"\" # Set data_path\n",
    "\n",
    "text_data = [data_path + \"/leg_train_text.txt\"]  # Sentence training data (spacy parsed)\n",
    "text_whole_data = [data_path + \"/leg_train_original.txt\"] # Whole sentence (not tokenised)\n",
    "labels_data = [data_path + \"/leg_train_label.txt\"] # Labels for sentences\n",
    "embed_vocab_data = data_path + \"/leg_embeddings_vocab.txt\" # Embedding vocab: words from training sentences \n",
    "# for which embeddings exist and have been extracted in embed_file. (If full, this is \"embed_vocab.txt\")\n",
    "full_vocab_data = data_path + \"/total_vocab.txt\" # Full sentence vocab. (\"total_vocab.txt\")\n",
    "\n",
    "\n",
    "txt_eos = \"</S>\" # Special characters\n",
    "lbl_sos = \"<l>\"\n",
    "lbl_eos = \"</l>\"\n",
    "embed_file = data_path + \"/leg_embeddings.txt\" # Embeddings file (full is \"embeddings.txt\")\n",
    "\n",
    "restore_path = None # Path to existing model, if fine-tuning or resuming training\n",
    "save_model = True # Set to True if you want to save model variables\n",
    "log_path = \"\" # Log directory\n",
    "save_path = \"\" # Save model path, only used if save_path is True\n",
    "log_freq = 100 # Show some outputs every log_freq training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "\n",
    "bi_directional = False\n",
    "num_dec_layers = 3 # If bi_directional is false, num_bi_layers is just number of encoder layers\n",
    "num_bi_layers = 3\n",
    "\n",
    "num_enc_units = 128\n",
    "num_dec_units = 128 # If uni-directional, then same as enc_units. If bi-directional, should be twice as big.\n",
    "\n",
    "beam_search = False # Whether to use beam_search decoding strategy\n",
    "beam_width = 4 # \n",
    "\n",
    "batch_size = 25\n",
    "forget_bias = 0\n",
    "dropout = 0.2\n",
    "max_gradient_norm = 1\n",
    "learning_rate = 0.002\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tf dataset: an iterator that returns batched data for training.\n",
    "\n",
    "def build_dataset(text_data, labels_data, embed_vocab_data, full_vocab_data, txt_eos, lbl_sos, lbl_eos, batch_size):\n",
    "\n",
    "    # Build the word to id lookup table from the text data. OOV words point at 0 = <unk> = random (but all same)\n",
    "    vocab_table = lookup_ops.index_table_from_file(embed_vocab_data, default_value=0)\n",
    "    \n",
    "    # Build a residual lookup table for all vocab, so can convert words back at end of process (evaluation only)\n",
    "    full_vocab_table = lookup_ops.index_table_from_file(full_vocab_data, default_value=0)\n",
    "\n",
    "    txt_eos_id = tf.cast(vocab_table.lookup(tf.constant(txt_eos)), tf.int32)\n",
    "    txt_full_eos_id = tf.cast(full_vocab_table.lookup(tf.constant(txt_eos)), tf.int32) # Probably not strictly necessary, since\n",
    "    # eos ends up in the same place in both vocab files.\n",
    "    lbl_sos_id = tf.cast(vocab_table.lookup(tf.constant(lbl_sos)), tf.int32)\n",
    "    lbl_eos_id = tf.cast(vocab_table.lookup(tf.constant(lbl_eos)), tf.int32)\n",
    "\n",
    "    # Read each line of the text file. Each line is a sentence (where text has been tokenised using spacy)\n",
    "    # NB can pass multiple files to TextLineDataset (so can prep data in batches)\n",
    "    sent_data = tf.data.TextLineDataset(text_data)\n",
    "    labels_data = tf.data.TextLineDataset(labels_data)\n",
    "\n",
    "    # For each line, split on white space\n",
    "    sent_data = sent_data.map(lambda string: tf.string_split([string]).values)\n",
    "    labels_data = labels_data.map(lambda label: tf.string_split([label]).values)\n",
    "    labels_data = labels_data.map(lambda label: tf.string_to_number(label, tf.int32))\n",
    "\n",
    "    # Lookup word ids (in the embedding vocab and in the full vocab)\n",
    "    embed_sent_data = sent_data.map(lambda token: tf.cast(vocab_table.lookup(token), tf.int32))\n",
    "    full_sent_data = sent_data.map(lambda token: tf.cast(full_vocab_table.lookup(token), tf.int32))\n",
    "\n",
    "    # Zip datasets together\n",
    "    sent_label_data = tf.data.Dataset.zip((full_sent_data, embed_sent_data, labels_data))\n",
    "    \n",
    "    # Create input dataset (labels prefixed by sos) and target dataset (labels suffixed with eos)\n",
    "    sent_label_data = sent_label_data.map(lambda full_words, embed_words, labels: (full_words, embed_words,\n",
    "                                                                                  tf.concat(([lbl_sos_id], labels), 0),\n",
    "                                                                                  tf.concat((labels, [lbl_eos_id]), 0),))\n",
    "\n",
    "    # Add seqeunce length\n",
    "    sent_label_data = sent_label_data.map(lambda full_words, embed_words, labels_in, labels_out: (full_words, embed_words,\n",
    "                                                                                                  tf.size(embed_words),\n",
    "                                                                                                  tf.size(labels_in),\n",
    "                                                                                                  labels_in, \n",
    "                                                                                                  labels_out))\n",
    "    \n",
    "    # Random shuffle\n",
    "    sent_label_data = sent_label_data.shuffle(buffer_size=5000)\n",
    "\n",
    "    # Batching the input, padding to the length of the longest sequence in the input. Can also bucket these. Form of dataset\n",
    "    # is: txt_ids_for_full_vocab, txt_ids_for_embed_vocab, text_size, label_size, labels_in, labels_out.\n",
    "    \n",
    "    batch_size = tf.constant(batch_size, tf.int64)\n",
    "    \n",
    "    batched_input = sent_label_data.padded_batch(batch_size, padded_shapes=(tf.TensorShape([None]),\n",
    "                                                                            tf.TensorShape([None]),\n",
    "                                                                            tf.TensorShape([]), \n",
    "                                                                            tf.TensorShape([]),\n",
    "                                                                            tf.TensorShape([None]), \n",
    "                                                                            tf.TensorShape([None])), \n",
    "                                                 padding_values=(txt_full_eos_id,\n",
    "                                                                 txt_eos_id,\n",
    "                                                                 0,\n",
    "                                                                 0,\n",
    "                                                                 lbl_eos_id, \n",
    "                                                                 lbl_eos_id))\n",
    "    iterator = batched_input.make_initializable_iterator()\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparatory step to create_emb_matrix. Each line of the embedding file is a word followed by a space delimited numbers forming\n",
    "# the vector. load_embed_txt splits on white space and builds a dictionary where keys are the words in the embedding file\n",
    "\n",
    "def load_embed_txt(embed_file):\n",
    "    emb_dict = dict()\n",
    "    with codecs.getreader(\"utf-8\")(tf.gfile.GFile(embed_file, 'rb')) as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split(\" \")\n",
    "            word = tokens[0]\n",
    "            vec = list(map(float, tokens[1:]))\n",
    "            emb_dict[word] = vec\n",
    "            emb_size = len(vec)\n",
    "    return emb_dict, emb_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix (numpy array of embeddings). Includes an <unk> value for oov words. These are the values that are\n",
    "# looked-up when the model is run.\n",
    "\n",
    "def create_emb_matrix(embed_file):\n",
    "    emb_dict, emb_size = load_embed_txt(embed_file)\n",
    "    mat = np.array([emb_dict[token] for token in emb_dict.keys()])\n",
    "    emb_mat = tf.convert_to_tensor(mat, dtype=tf.float32)\n",
    "    return emb_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hack to help with the input to the decoder. Creates a matrix where keys and values are just integers in single item lists.\n",
    "\n",
    "def create_dec_matrix(num):\n",
    "    dec_dict = {}\n",
    "    for i in range(num):\n",
    "        dec_dict[i] = [i]\n",
    "    mat = np.array([dec_dict[token] for token in dec_dict.keys()])\n",
    "    dec_mat = tf.convert_to_tensor(mat, dtype=tf.float32)\n",
    "    return dec_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the id to vocab dictionary (reverse of the vocab lookup). This is for the \"embed vocab\" (i.e. where lots of words are\n",
    "# still mapped to <unk>)). This assumes there is both: an \"embed vocab file\", a file of the vocab for which embeddings exist and\n",
    "# an embed file. Recall unk and special characters are included in the vocab file, so no need to manaully add to the dictionary.\n",
    "# The words are just set out on each line of the file, so \"strip\" / \"split\" is a bit overkill but works well enough.\n",
    "\n",
    "def ids_to_embed_vocab(embed_vocab_data):\n",
    "    embed_vocab_dict = {}\n",
    "    with codecs.getreader(\"utf-8\")(tf.gfile.GFile(embed_vocab_data, 'rb')) as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            tokens = line.strip().split(\" \")\n",
    "            word = tokens[0]\n",
    "            embed_vocab_dict[count] = word\n",
    "            count += 1\n",
    "    return embed_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the id to vocab dictionary (reverse of the vocab lookup). This is for the full vocab. This is a hack, not really\n",
    "# necessary for the model but allows you to read the outputs easier (otherwise you would be left with lots of \"unks\" in the\n",
    "# final output.) We don't compute with these ids, they are just preserved through the batch input so we know what words went in.\n",
    "\n",
    "def ids_to_full_vocab(full_vocab_data):\n",
    "    full_vocab_dict = {}\n",
    "    with codecs.getreader(\"utf-8\")(tf.gfile.GFile(full_vocab_data, 'rb')) as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            tokens = line.strip().split(\" \")\n",
    "            word = tokens[0]\n",
    "            full_vocab_dict[count] = word\n",
    "            count += 1\n",
    "    return full_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single LSTM cell instance with dropout option.\n",
    "\n",
    "def single_cell(num_units, forget_bias, dropout, name):\n",
    "    single_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, forget_bias=forget_bias, name=name)\n",
    "    if dropout > 0.0:\n",
    "        single_cell = tf.nn.rnn_cell.DropoutWrapper(cell=single_cell, input_keep_prob=(1.0 - dropout))\n",
    "    return single_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer RNN definition. The \"direction\" argument is just to help with naming when using bi-directional model.\n",
    "\n",
    "def RNN_cell(num_layers, num_units, forget_bias, dropout, direction):\n",
    "    if num_layers == 1:\n",
    "        cell_name = direction + \"_LSTM_layer\"\n",
    "        rnn_cell = single_cell(num_units, forget_bias, dropout, cell_name)\n",
    "    else:\n",
    "        cell_list = []\n",
    "        for i in range(num_layers):\n",
    "            cell_name = direction + \"_LSTM_layer_\" + str(i)\n",
    "            cell = single_cell(num_units, forget_bias, dropout, cell_name)\n",
    "            cell_list.append(cell)\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "    return rnn_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose cells and get output\n",
    "\n",
    "def build_encoder(encoder_emb_inp, num_layers, num_units, forget_bias, dropout, txt_size, bi_directional):\n",
    "    if bi_directional == True:\n",
    "        fw_cell = RNN_cell(num_layers, num_units, forget_bias, dropout, \"enc_fw\")\n",
    "        bw_cell = RNN_cell(num_layers, num_units, forget_bias, dropout, \"enc_bw\")\n",
    "        outputs, (encoder_fw_state, encoder_bw_state) = tf.nn.bidirectional_dynamic_rnn(fw_cell,\n",
    "                                                                                              bw_cell,\n",
    "                                                                                              encoder_emb_inp,\n",
    "                                                                                              sequence_length=txt_size,\n",
    "                                                                                              time_major=False,\n",
    "                                                                                              dtype = tf.float32)\n",
    "        \n",
    "        encoder_outputs = tf.concat(outputs, 2)\n",
    "        \n",
    "        if isinstance(encoder_fw_state, tuple) and isinstance(encoder_fw_state[0], tf.contrib.rnn.LSTMStateTuple):  # MultiLstmCell\n",
    "            encoder_state = tuple(map(\n",
    "                lambda fw_state, bw_state: tf.contrib.rnn.LSTMStateTuple(\n",
    "                    c=tf.concat((fw_state.c, bw_state.c), 1,\n",
    "                                name=\"bidirectional_concat_c\"),\n",
    "                    h=tf.concat((fw_state.h, bw_state.h), 1,\n",
    "                                name=\"bidirectional_concat_h\")),\n",
    "                encoder_fw_state, encoder_bw_state))\n",
    "        else:\n",
    "            encoder_state = tf.concat(\n",
    "                (encoder_fw_state, encoder_bw_state), 1,\n",
    "                name=\"bidirectional_state_concat\")\n",
    "    if bi_directional == False:\n",
    "        encoder_cells = RNN_cell(num_layers, num_units, forget_bias, dropout, \"enc_fw\")\n",
    "        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cells, \n",
    "                                                           encoder_emb_inp, \n",
    "                                                           sequence_length=txt_size, \n",
    "                                                           time_major=False, \n",
    "                                                           dtype = tf.float32)\n",
    "    \n",
    "    return encoder_outputs, encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, learning_rate, dropout, max_gradient_norm, num_enc_units, num_dec_units, num_dec_layers, \n",
    "                 num_bi_layers, bi_directional, forget_bias, embed_words, full_words, txt_size, labels_size, \n",
    "                 labels_in, labels_out):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout = dropout\n",
    "        self.num_enc_units = num_enc_units\n",
    "        self.forget_bias = forget_bias\n",
    "        self.max_gradient_norm = max_gradient_norm\n",
    "        \n",
    "        self.words_in = embed_words\n",
    "        self.full_words_in = full_words\n",
    "        \n",
    "        with tf.variable_scope(\"main\", initializer=xavier_initializer()):\n",
    "            \n",
    "            # Inputs\n",
    "            \n",
    "            mask_labels = tf.sequence_mask(labels_size, dtype=tf.int32) # To mask the padded input\n",
    "            labels_in = labels_in * mask_labels\n",
    "            \n",
    "            self.labels_out = labels_out\n",
    "            self.mask_labels = mask_labels\n",
    "            \n",
    "            encoder_emb_inp = tf.nn.embedding_lookup(emb_mat, embed_words) # Encoder embedding lookup\n",
    "            decoder_emb_inp = tf.nn.embedding_lookup(dec_mat, labels_in) # Decoder embedding lookup (easiest way to get it in\n",
    "            # right shape)\n",
    "            \n",
    "            # Encoder definition (by default, encoder_state is just the final state). Encoder can be multi-layers and\n",
    "            # bi-directional\n",
    "            \n",
    "            encoder_outputs, encoder_state = build_encoder(encoder_emb_inp, num_bi_layers, num_enc_units, forget_bias, \n",
    "                                                           dropout, txt_size, bi_directional)\n",
    "            \n",
    "            # Decoder definition. Number of decoder layers is the same as the number of encoder layers, but needed be. The \n",
    "            # helper is defined seperately and can be adjusted for greedy / beam decoding at inference.\n",
    "            \n",
    "            decoder_cells = RNN_cell(num_dec_layers, num_dec_units, forget_bias, dropout, \"dec\")\n",
    "            \n",
    "            helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, \n",
    "                                                       labels_size, \n",
    "                                                       time_major=False)\n",
    "            \n",
    "            # Output layer which takes decoder output and maps to 3 categories (0,1,2) - these are the same as the target labels.\n",
    "            # Recall 2 just maps to </l>, which is the prediction for </s>\n",
    "            \n",
    "            output_layer = layers_core.Dense(3, use_bias=False, name=\"output_projection\")\n",
    "            \n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cells, helper, encoder_state, output_layer)\n",
    "            \n",
    "            outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder,\n",
    "                                                                                output_time_major=False)\n",
    "            \n",
    "            # Decoder just runs until it gets to the end, but could impose a max length (e.g. length of labels)\n",
    "            \n",
    "            # Calculate loss: By logits we just mean the outputs of the decoder (after output_layer). crossent takes normalised\n",
    "            # output probability prediction for each class (i.e. the softmax of the logits) and takes cross-entropy with the \n",
    "            # actual labels.\n",
    "            \n",
    "            self.logits = outputs[0]\n",
    "            crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels_out, logits=self.logits)\n",
    "            self.loss = tf.reduce_sum(crossent * tf.cast(mask_labels, tf.float32)) / tf.cast(batch_size, tf.float32)\n",
    "            \n",
    "            # Gradients: Gradients extracted (variables is all trainable variable), then \"clipped\" by global norm to prevent\n",
    "            # \"exploding\" gradients, before being applied to the variables.\n",
    "                      \n",
    "            opt = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            gradients, variables = zip(*opt.compute_gradients(self.loss))\n",
    "            \n",
    "            clipped_grads, grad_norm = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
    "            \n",
    "            self.grad_norm = grad_norm\n",
    "            \n",
    "            # Training module: \"preds\" is just a hack so we can see some rough predictions. Take the argmax of the logits. Only\n",
    "            # used to interogate output, doesn't feed into computation (helper does this job)\n",
    "            \n",
    "            self.train = opt.apply_gradients(zip(clipped_grads, variables))\n",
    "            self.preds = tf.argmax(self.logits, axis=2)\n",
    "            \n",
    "            # Summaries: Tensorflow summaries\n",
    "            \n",
    "            self.make_summaries(self.learning_rate, self.dropout, self.loss, self.grad_norm)\n",
    "            \n",
    "    def gradient_clip(gradients, max_gradient_norm):\n",
    "        \n",
    "        clipped_grads, grad_norm = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
    "        \n",
    "        grad_norm_summary = [tf.summary.scalar(\"grad_norm\", grad_norm)]\n",
    "        \n",
    "        return clipped_grads, gradient_norm_summary, grad_norm\n",
    "            \n",
    "    def make_summaries(self, learning_rate, dropout, loss, grad_norm):\n",
    "        \n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "        tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "        tf.summary.scalar(\"dropout\", dropout)\n",
    "        tf.summary.scalar(\"grad_norm\", grad_norm)\n",
    "        \n",
    "        self.merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the graph\n",
    "\n",
    "with tf.Graph().as_default(): \n",
    "\n",
    "    iterator = build_dataset(text_data, labels_data, embed_vocab_data, full_vocab_data, txt_eos, lbl_sos, lbl_eos, batch_size)\n",
    "    \n",
    "    emb_mat = create_emb_matrix(embed_file)\n",
    "    dec_mat = create_dec_matrix(4)\n",
    "    \n",
    "    random_embeddings = np.random.uniform(low=-1, high=1, size=(4,300)) # A random choice for unk and other special characters\n",
    "    embeddings = tf.Variable(tf.convert_to_tensor(random_embeddings, dtype=tf.float32), name=\"saved_embeddings\")\n",
    "    emb_mat = tf.concat((embeddings, emb_mat), 0)\n",
    "    \n",
    "    ids_to_embed_vocab = ids_to_embed_vocab(embed_vocab_data)\n",
    "    ids_to_full_vocab = ids_to_full_vocab(full_vocab_data)\n",
    "    \n",
    "    # A call to the iterator for inputs\n",
    "    \n",
    "    full_words_, embed_words_, txt_size_, label_size_, labels_in_, labels_out_ = iterator.get_next()\n",
    "    \n",
    "    # Model instantiation\n",
    "    \n",
    "    model = Model(learning_rate, dropout, max_gradient_norm, num_enc_units, num_dec_units, num_dec_layers, num_bi_layers, bi_directional,\n",
    "                  forget_bias, embed_words_, full_words_, txt_size_, label_size_, labels_in_, labels_out_)\n",
    "    \n",
    "    # Initialise variables\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    saver = tf.train.Saver() # Saver for variables. Not full graph.\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(log_path, sess.graph)\n",
    "        counter = 0\n",
    "        \n",
    "        # Restore variables if present.\n",
    "        if restore_path == None:\n",
    "            sess.run(init)\n",
    "        else:\n",
    "            saver.restore(sess, restore_path)\n",
    "            print(\"Model restored.\")\n",
    "        \n",
    "        # Initialise the vocab tables\n",
    "        sess.run(tf.tables_initializer())\n",
    "        \n",
    "        # Training loop.\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            epoch_start = time.time()\n",
    "            sess.run(iterator.initializer)\n",
    "            while True:\n",
    "                try:\n",
    "                    _, summary, loss = sess.run([model.train, model.merged, model.loss])\n",
    "                    \n",
    "                    train_writer.add_summary(summary, counter)\n",
    "                    train_writer.flush()\n",
    "                    counter += 1 # Count batches\n",
    "                    losses.append(loss) # Counter for epoch loss\n",
    "                    \n",
    "                    if counter % log_freq == 0:\n",
    "                        \n",
    "                        # Get the values from model\n",
    "                        preds, full_words_in, labels_out, mask_labels = sess.run([model.preds,\n",
    "                                                                                  model.full_words_in, \n",
    "                                                                                  model.labels_out, \n",
    "                                                                                  model.mask_labels])\n",
    "\n",
    "                        # pick one of the entries in the current batch\n",
    "                        j = np.random.randint(0, batch_size)\n",
    "                            \n",
    "                        full_sent = []\n",
    "                        target_sent = []\n",
    "                        predicted_sent = []\n",
    "\n",
    "                        for i in range(len(full_words_in[j])):\n",
    "                            if mask_labels[j][i] == 1:\n",
    "                                full_sent.append(ids_to_full_vocab[full_words_in[j][i]])\n",
    "                                if preds[j][i] != 0:\n",
    "                                    predicted_sent.append(ids_to_full_vocab[full_words_in[j][i]])\n",
    "                                if labels_out[j][i] != 0:\n",
    "                                    target_sent.append(ids_to_full_vocab[full_words_in[j][i]])\n",
    "\n",
    "                        print(\"Input sentence is:\")\n",
    "                        print(\" \".join(full_sent))\n",
    "                        print(\"Target sentence is:\")\n",
    "                        print(\" \".join(target_sent))\n",
    "                        print(\"Predicted sentence is:\")\n",
    "                        print(\" \".join(predicted_sent))\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    \n",
    "                    average_loss = sum(losses) / len(losses)\n",
    "                    elapsed_time = (time.time() - epoch_start)\n",
    "                    print(\"Epoch run time: %s\" % elapsed_time)\n",
    "                    print(\"Average epoch loss: %s\" % average_loss)\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "        if save_model == True:\n",
    "            saver.save(sess, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
