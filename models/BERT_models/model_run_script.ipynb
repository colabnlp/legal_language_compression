{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT run model script\n",
    "\n",
    "The purpose of this script is to train and test the BERT models. This script is designed to be used in Google Colaboratory in the sense that:\n",
    "\n",
    "- it assumes data will be loaded from both Google Drive and Google Storage Buckets\n",
    "- it assumes the script will be executed on a Colabotory GPU\n",
    "\n",
    "One training epoch on the legislative data (with batch size 25) take about 40 seconds on a Colaboratory GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5186,
     "status": "ok",
     "timestamp": 1564139206409,
     "user": {
      "displayName": "Richard Batstone",
      "photoUrl": "https://lh6.googleusercontent.com/-TnPwXZfZr68/AAAAAAAAAAI/AAAAAAAAAMA/FW6bTWy9_is/s64/photo.jpg",
      "userId": "04086047539127212340"
     },
     "user_tz": -60
    },
    "id": "r9hzH1lke221",
    "outputId": "632bfe59-a632-4fa4-b541-f287369fcf9e"
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "!pip install bert-tensorflow\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import tensorflow_hub as hub\n",
    "import sys\n",
    "import bert\n",
    "import numpy as np\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2981,
     "status": "ok",
     "timestamp": 1564139213228,
     "user": {
      "displayName": "Richard Batstone",
      "photoUrl": "https://lh6.googleusercontent.com/-TnPwXZfZr68/AAAAAAAAAAI/AAAAAAAAAMA/FW6bTWy9_is/s64/photo.jpg",
      "userId": "04086047539127212340"
     },
     "user_tz": -60
    },
    "id": "170TqhvQfO91",
    "outputId": "91708d8f-d4b4-4947-d8d5-d040bfb6d0cb"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 679,
     "status": "ok",
     "timestamp": 1564139216741,
     "user": {
      "displayName": "Richard Batstone",
      "photoUrl": "https://lh6.googleusercontent.com/-TnPwXZfZr68/AAAAAAAAAAI/AAAAAAAAAMA/FW6bTWy9_is/s64/photo.jpg",
      "userId": "04086047539127212340"
     },
     "user_tz": -60
    },
    "id": "JNoTGgTCfZdr",
    "outputId": "6bdf0edc-0c54-4463-8fd0-eb28c6a80874"
   },
   "outputs": [],
   "source": [
    "# Import local modules - if taken to production, these would be properly packaged...\n",
    "\n",
    "sys.path.insert(0, '/content/gdrive/My Drive/') # Set path to Google Drive location (location of beam_search and model modules)\n",
    "import beam_search\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4035,
     "status": "ok",
     "timestamp": 1564139223531,
     "user": {
      "displayName": "Richard Batstone",
      "photoUrl": "https://lh6.googleusercontent.com/-TnPwXZfZr68/AAAAAAAAAAI/AAAAAAAAAMA/FW6bTWy9_is/s64/photo.jpg",
      "userId": "04086047539127212340"
     },
     "user_tz": -60
    },
    "id": "mTansETAfnPY",
    "outputId": "e2461670-8435-48c8-f682-d0cd84dd31e2"
   },
   "outputs": [],
   "source": [
    "# Check GPU runtime\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqiJFS52f8p_"
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "\n",
    "# Encoder (BERT loaded from tfhub)\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\" # This is a path to an uncased (all lowercase) version of BERT\n",
    "\n",
    "# Decoder\n",
    "params = {\n",
    "    \"num_hidden_layers\":6,\n",
    "    \"hidden_size\":6,\n",
    "    \"num_heads\":3,\n",
    "    \"filter_size\":512,\n",
    "    \"relu_dropout\":0.1,\n",
    "    \"allow_ffn_pad\":True,\n",
    "    \"layer_postprocess_dropout\":0.1,\n",
    "    \"attention_dropout\":0.1,\n",
    "    \"initializer_gain\":1.0,\n",
    "    \"label_smoothing\":0.1,\n",
    "    \"beam_size\":4,\n",
    "    \"alpha\":1,\n",
    "    \"bucket\":'BUCKET_NAME', # Set Google Storage Bucket name\n",
    "    \"task_data_dir\":'gs://BUCKET_NAME/', # Set Google Storage Bucket name\n",
    "    \"output_dir\":'gs://{}/XXXXX/' # Set Google Storage Bucket output directory\n",
    "}\n",
    "\n",
    "# Compute train and warmup steps from batch size\n",
    "# These transfer learning hyperparameters are taken from the 'BERT fine-tuning' notebook \n",
    "# (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 25\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 10\n",
    "# Warmup is a period of time where the learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 1000\n",
    "SAVE_SUMMARY_STEPS = 500\n",
    "MAX_SEQ_LENGTH = 128 # This should match the max seq length set when creating the input file\n",
    "\n",
    "TRAIN_FILE = '' # Set data path\n",
    "VALIDATION_FILE = '' # Set data path\n",
    "TEST_FILE = '' # Set data path\n",
    "\n",
    "NUM_TRAINING_EXAMPLES = 750 # Set number of training examples\n",
    "\n",
    "num_train_steps = int(NUM_TRAINING_EXAMPLES/ BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5530,
     "status": "ok",
     "timestamp": 1564139293768,
     "user": {
      "displayName": "Richard Batstone",
      "photoUrl": "https://lh6.googleusercontent.com/-TnPwXZfZr68/AAAAAAAAAAI/AAAAAAAAAMA/FW6bTWy9_is/s64/photo.jpg",
      "userId": "04086047539127212340"
     },
     "user_tz": -60
    },
    "id": "sElDxvNnfsPP",
    "outputId": "5316509e-0225-48fb-d7a3-887c15274508"
   },
   "outputs": [],
   "source": [
    "# Set data input and output directory (these are taken from params)\n",
    "\n",
    "TASK_DATA_DIR = params['task_data_dir']\n",
    "print('***** Task data directory: {} *****'.format(TASK_DATA_DIR))\n",
    "!gsutil ls $TASK_DATA_DIR\n",
    "\n",
    "BUCKET = params['bucket']\n",
    "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
    "OUTPUT_DIR = params['output_dir'].format(BUCKET)\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGKP0Wd8iTy7"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/tensorflow/models/tree/master/official/transformer\n",
    "# Define the transformer encoder/decoder model\n",
    "\n",
    "def create_model(is_predicting, train, input_ids, input_mask, segment_ids, labels, r_id):\n",
    "  \"\"\"Creates transformer model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  \n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  \n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  \n",
    "  output_layer = bert_outputs[\"sequence_output\"]\n",
    "  \n",
    "  initializer = tf.variance_scaling_initializer(params[\"initializer_gain\"],\n",
    "                                                mode=\"fan_avg\",\n",
    "                                                distribution=\"uniform\")\n",
    "  \n",
    "  with tf.variable_scope(\"Decoder_transformer_loss\", initializer=initializer):\n",
    "    \n",
    "    decoder_embeddings = model.DecoderEmbeddings()\n",
    "    decoder_stack = model.DecoderStack(params, train)    \n",
    "    attention_bias = model.get_padding_bias(input_mask)\n",
    "    \n",
    "    if is_predicting:\n",
    "      predictions = predict(output_layer, attention_bias, decoder_embeddings, decoder_stack, params)\n",
    "      return predictions\n",
    "    \n",
    "    else:\n",
    "      logits = model.decode(labels, output_layer, attention_bias, decoder_embeddings, decoder_stack, train, params)\n",
    "      xentropy, weights = model.padded_cross_entropy_loss(logits, \n",
    "                                                    labels, \n",
    "                                                    params[\"label_smoothing\"], \n",
    "                                                    6)\n",
    "      loss = tf.reduce_sum(xentropy) / tf.reduce_sum(weights)\n",
    "      predictions = tf.argmax(logits, axis=-1)\n",
    "      return (loss, predictions, xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKZM6QV-UP6u"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/tensorflow/models/tree/master/official/transformer\n",
    "# Prediction time function (implements beam search decoding strategy)\n",
    "\n",
    "def predict(encoder_outputs, encoder_decoder_attention_bias, decoder_embeddings, decoder_stack, params):\n",
    "    \"\"\"Return predicted sequence.\"\"\"\n",
    "    batch_size = tf.shape(encoder_outputs)[0]\n",
    "    input_length = tf.shape(encoder_outputs)[1]\n",
    "    max_decode_length = input_length\n",
    "\n",
    "    symbols_to_logits_fn = model._get_symbols_to_logits_fn(max_decode_length, decoder_embeddings, decoder_stack, params)\n",
    "\n",
    "    # Create initial set of IDs that will be passed into symbols_to_logits_fn.\n",
    "    initial_ids = tf.zeros([batch_size], dtype=tf.int32)\n",
    "\n",
    "    # Create cache storing decoder attention values for each layer.\n",
    "    cache = {\n",
    "        \"layer_%d\" % layer: {\n",
    "            \"k\": tf.zeros([batch_size, 0, params[\"hidden_size\"]]),\n",
    "            \"v\": tf.zeros([batch_size, 0, params[\"hidden_size\"]]),\n",
    "        } for layer in range(params[\"num_hidden_layers\"])}\n",
    "\n",
    "    # Add encoder output and attention bias to the cache.\n",
    "    cache[\"encoder_outputs\"] = encoder_outputs\n",
    "    cache[\"encoder_decoder_attention_bias\"] = encoder_decoder_attention_bias\n",
    "\n",
    "    # Use beam search to find the top beam_size sequences and scores.\n",
    "    decoded_ids, scores = beam_search.sequence_beam_search(\n",
    "        symbols_to_logits_fn=symbols_to_logits_fn,\n",
    "        initial_ids=initial_ids,\n",
    "        initial_cache=cache,\n",
    "        vocab_size=6,\n",
    "        beam_size=params[\"beam_size\"],\n",
    "        alpha=params[\"alpha\"],\n",
    "        max_decode_length=max_decode_length,\n",
    "        eos_id=6,\n",
    "        decoder_embeddings=decoder_embeddings,\n",
    "        decoder_stack=decoder_stack)\n",
    "    \n",
    "    # setting vocab_size to 6 for labels number. EOS is 4\n",
    "\n",
    "    # Get the top sequence for each batch element\n",
    "    top_decoded_ids = decoded_ids[:, 0, 1:]\n",
    "    top_scores = scores[:, 0]\n",
    "\n",
    "    return {\"outputs\": top_decoded_ids, \"scores\": top_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8hmQXmzifgh"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb\n",
    "# model_fn_builder actually creates the model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "\n",
    "def model_fn_builder(learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, mode, params):\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    labels = features[\"label_ids\"]\n",
    "    try:\n",
    "      r_id = features[\"r_id\"]\n",
    "    except:\n",
    "      r_id = []\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "      train = True\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, train, input_ids, input_mask, segment_ids, labels, r_id)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      train = False\n",
    "      preds = create_model(\n",
    "        is_predicting, train, input_ids, input_mask, segment_ids, labels, r_id)\n",
    "\n",
    "      predictions = {\n",
    "          'true_labels':labels,\n",
    "          'predicted_labels': preds[\"outputs\"],\n",
    "          'r_id': r_id\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2T7oJG2jmfJ"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb\n",
    "\n",
    "def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
    "                                drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  name_to_features = {\n",
    "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"label_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"r_id\": tf.FixedLenFeature([1], tf.int64),\n",
    "      \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
    "  }\n",
    "\n",
    "  def _decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "      t = example[name]\n",
    "      if t.dtype == tf.int64:\n",
    "        t = tf.to_int32(t)\n",
    "      example[name] = t\n",
    "\n",
    "    return example\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    d = tf.data.TFRecordDataset(input_file)\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.apply(\n",
    "        tf.contrib.data.map_and_batch(\n",
    "            lambda record: _decode_record(record, name_to_features),\n",
    "            batch_size=batch_size,\n",
    "            drop_remainder=drop_remainder))\n",
    "\n",
    "    return d\n",
    "\n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHh82xQvjGf2"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb\n",
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oepu5BmZjghH"
   },
   "outputs": [],
   "source": [
    "# Adapted from https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb\n",
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "\n",
    "train_input_fn = file_based_input_fn_builder(\n",
    "    input_file=LEG_TRAIN_FILE,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 391129,
     "status": "ok",
     "timestamp": 1564139723858,
     "user": {
      "displayName": "Richard Batstone",
      "photoUrl": "https://lh6.googleusercontent.com/-TnPwXZfZr68/AAAAAAAAAAI/AAAAAAAAAMA/FW6bTWy9_is/s64/photo.jpg",
      "userId": "04086047539127212340"
     },
     "user_tz": -60
    },
    "id": "e6V9b9C7jqhe",
    "outputId": "56aefeb7-a792-4e3c-e045-435220751341"
   },
   "outputs": [],
   "source": [
    "# Run training\n",
    "\n",
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh4OZLhQjtRt"
   },
   "outputs": [],
   "source": [
    "# Create an input function for testing. drop_remainder = True for using TPUs.\n",
    "\n",
    "pred_input_fn = file_based_input_fn_builder(\n",
    "    input_file=TEST_FILE,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "predictions = estimator.predict(pred_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPRKD4Zljys8"
   },
   "outputs": [],
   "source": [
    "# Run predictions and save them in true_label and predicted_labels\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "r_ids = []\n",
    "\n",
    "for prediction in predictions:\n",
    "  true_labels.append(prediction['true_labels'])\n",
    "  predicted_labels.append(prediction['predicted_labels'])\n",
    "  r_ids.append(prediction['r_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1564140521442,
     "user": {
      "displayName": "Richard Batstone",
      "photoUrl": "https://lh6.googleusercontent.com/-TnPwXZfZr68/AAAAAAAAAAI/AAAAAAAAAMA/FW6bTWy9_is/s64/photo.jpg",
      "userId": "04086047539127212340"
     },
     "user_tz": -60
    },
    "id": "wmvt1lnej1pd",
    "outputId": "a1d0fdad-7649-410d-a9a5-5027f1f2c487"
   },
   "outputs": [],
   "source": [
    "# Run the evalutation\n",
    "\n",
    "# Get rid of padding (truncate preds by input length)\n",
    "def remove_padding(true_labels, predicted_labels):\n",
    "  non_pad_labels = []\n",
    "  non_pad_preds = []\n",
    "  for i in range(len(true_labels)):\n",
    "    eos = true_labels[i].argmax(axis=0)\n",
    "    non_pad_labels.append(true_labels[i][1:eos])\n",
    "    non_pad_preds.append(predicted_labels[i][1:eos])\n",
    "  return non_pad_labels, non_pad_preds\n",
    "\n",
    "non_pad_labels, non_pad_preds = remove_padding(true_labels, predicted_labels)\n",
    "\n",
    "# Raw accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(non_pad_labels)):\n",
    "  for j in range(len(non_pad_labels[i])):\n",
    "    total += 1    \n",
    "    if non_pad_labels[i][j] == non_pad_preds[i][j]:\n",
    "      correct += 1\n",
    "print(correct/total)\n",
    "\n",
    "labels_concat = np.concatenate((non_pad_labels))\n",
    "preds_concat = np.concatenate((non_pad_preds))\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix(labels_concat, preds_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DtHhZEYh_nz"
   },
   "outputs": [],
   "source": [
    "# Export results\n",
    "\n",
    "import pickle\n",
    "\n",
    "results = {}\n",
    "r_ids_nums = []\n",
    "for i in r_ids:\n",
    "  r_ids_nums.append(i[0])\n",
    "results['true_labels'] = non_pad_labels\n",
    "results['predicted_labels'] = non_pad_preds\n",
    "results['r_ids'] = r_ids_nums\n",
    "\n",
    "results_file = \"\" # Set results file\n",
    "\n",
    "with open(results_file, 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(results, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7u-UGgG7X882"
   },
   "outputs": [],
   "source": [
    "# Download results\n",
    "\n",
    "from google.colab import files\n",
    "files.download(results_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_compression_clean.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
