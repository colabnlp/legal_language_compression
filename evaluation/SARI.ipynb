{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARI script\n",
    "\n",
    "A script to run the SARI evaluation metric. Model outputs must be reconstructed before the SARI score is calculated and the script provides different reconstruction functions depending on the model which is being evaluated.\n",
    "\n",
    "SARI is implemented using an adapted version of Wei Coco Xu's script: <https://github.com/cocoxu/simplification>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SARI import SARIsent\n",
    "from nltk import word_tokenize\n",
    "from LSTM_reconstruct import LSTM_reconstruct\n",
    "from BERT_reconstruct import BERT_reconstruct\n",
    "from BERT_reconstruct import BERT_rules_reconstruct\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data\n",
    "\n",
    "def read_data(test_file):\n",
    "    with open(test_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"\" # Path to test output\n",
    "test_data = read_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output is from a LSTM model: reconstruct those outputs\n",
    "\n",
    "target_reconstructions = []\n",
    "pred_reconstructions = []\n",
    "\n",
    "for i in range(len(test_data['predictions'])):\n",
    "    pred_recon, target_recon = LSTM_reconstruct(test_data['predictions'][i],\n",
    "                                           test_data['targets'][i])\n",
    "    pred_reconstructions.append(pred_recon)\n",
    "    target_reconstructions.append(target_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output is from a BERT_rules ensemble model: reconstruct those outputs\n",
    "\n",
    "rules_data = read_data(\"\") # Path to rules output\n",
    "\n",
    "pred_reconstructions, target_reconstructions, originals = BERT_rules_reconstruct(test_data, rules_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output is from a Rules based model: reconstruct those outputs\n",
    "\n",
    "target_reconstructions = []\n",
    "pred_reconstructions = []\n",
    "originals = []\n",
    "\n",
    "for i in range(len(test_data['predictions'])):\n",
    "    target_reconstructions.append(test_data['targets'][i])\n",
    "    pred_reconstructions.append(test_data['predictions'][i])\n",
    "    originals.append(test_data['originals'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output is from a BERT model: reconstruct those outputs\n",
    "\n",
    "pred_reconstructions, target_reconstructions, originals = BERT_reconstruct(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process original inputs (LSTM only)\n",
    "\n",
    "originals = []\n",
    "\n",
    "for i in range(len(test_data['predictions'])):\n",
    "    original = test_data['wholeInput'][i]\n",
    "    if len(original) != 0:\n",
    "        if str(original)[0:2] == \"b'\" and str(original)[-1] == \"'\":\n",
    "            original = str(original)[2:-1]\n",
    "            originals.append(original)\n",
    "        elif str(original)[0:2] == 'b\"' and str(original)[-1] == '\"':\n",
    "            original = str(original)[2:-1]\n",
    "            originals.append(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SARI scores\n",
    "\n",
    "count = len(pred_reconstructions)\n",
    "rolling_SARI = 0\n",
    "\n",
    "for i in range(len(pred_reconstructions)):\n",
    "    token_target = word_tokenize(target_reconstructions[i])\n",
    "    token_prediction = word_tokenize(pred_reconstructions[i])\n",
    "    token_original = word_tokenize(originals[i])\n",
    "    sentence_SARI, _, _, _ = SARIsent(token_original, token_prediction, [token_target])\n",
    "    rolling_SARI += sentence_SARI\n",
    "    \n",
    "average_SARI = rolling_SARI/count\n",
    "print(average_SARI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
