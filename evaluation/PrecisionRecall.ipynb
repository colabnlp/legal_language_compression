{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision / Recall / F1 script\n",
    "\n",
    "A script to calculate precision recall and F1 score. This script also calculate the compression ratio and string-for-string match. The script provides different read functions depending on the model which is being evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from LSTM_reconstruct import remove_padding\n",
    "from LSTM_reconstruct import LSTM_reconstruct\n",
    "from BERT_reconstruct import BERT_reconstruct\n",
    "from BERT_reconstruct import BERT_rules_reconstruct\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data\n",
    "\n",
    "def read_data(test_file):\n",
    "    with open(test_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"\" # Path to test output\n",
    "test_data = read_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output is from a BERT model\n",
    "\n",
    "non_pad_labels = [i for i in test_data['true_labels']]\n",
    "non_pad_preds = [i for i in test_data['predicted_labels']]\n",
    "\n",
    "pred_reconstructions, target_reconstructions, originals = BERT_reconstruct(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output is from a Rules based model\n",
    "\n",
    "non_pad_labels = [i for i in test_data['true_labels']]\n",
    "non_pad_preds = [i for i in test_data['predicted_labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output is from a BERT_rules ensemble model\n",
    "\n",
    "rules_data = read_data(\"\") # Path to rules output\n",
    "\n",
    "pred_reconstructions, target_reconstructions, originals = BERT_rules_reconstruct(test_data, rules_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output is from a LSTM model\n",
    "\n",
    "true_labels = [i for i in test_data['target_labels']]\n",
    "predicted_labels = [i for i in test_data['predicted_labels']]\n",
    "\n",
    "non_pad_labels, non_pad_preds = remove_padding(true_labels, predicted_labels)\n",
    "\n",
    "target_reconstructions = []\n",
    "pred_reconstructions = []\n",
    "\n",
    "for i in range(len(test_data['predictions'])):\n",
    "    pred_recon, target_recon = LSTM_reconstruct(test_data['predictions'][i],\n",
    "                                           test_data['targets'][i])\n",
    "    pred_reconstructions.append(pred_recon)\n",
    "    target_reconstructions.append(target_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy and confusion matrix\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(non_pad_labels)):\n",
    "    for j in range(len(non_pad_labels[i])):\n",
    "        total += 1    \n",
    "        if non_pad_labels[i][j] == non_pad_preds[i][j]:\n",
    "            correct += 1\n",
    "\n",
    "print(\"Accuracy:\")\n",
    "print(correct/total)\n",
    "\n",
    "labels_concat = np.concatenate((non_pad_labels))\n",
    "preds_concat = np.concatenate((non_pad_preds))\n",
    "\n",
    "# Confusion matrix\n",
    "\n",
    "confusion_matrix(labels_concat, preds_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision and recall (retained tokens) - hard coded from confusion matrix outputs (because of occasional label noise)\n",
    "\n",
    "precision = 939 / (939+272)\n",
    "recall = 939 / (939+491)\n",
    "f1 = (2*precision*recall)/(precision+recall)\n",
    "\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String for string match (LSTM)\n",
    "\n",
    "matches = 0\n",
    "total = 0\n",
    "for i in range(len(test_data['predictions'])):\n",
    "    if test_data['predictions'][i] == test_data['targets'][i]:\n",
    "        matches += 1\n",
    "    total += 1\n",
    "print(\"Total matches:\")\n",
    "print(matches)\n",
    "print(matches/(total/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String for string match (BERT)\n",
    "\n",
    "matches = 0\n",
    "total = 0\n",
    "for i in range(len(pred_reconstructions)):\n",
    "    if pred_reconstructions[i] == target_reconstructions[i]:\n",
    "        matches += 1\n",
    "    total += 1\n",
    "print(\"Total matches:\")\n",
    "print(matches)\n",
    "print(matches/(total/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression ratio BERT\n",
    "\n",
    "crs = 0\n",
    "\n",
    "for i in range(len(pred_reconstructions)):\n",
    "    token_original = word_tokenize(originals[i])\n",
    "    token_prediction = word_tokenize(pred_reconstructions[i])\n",
    "    cr = len(token_prediction)/len(token_original)\n",
    "    crs += cr\n",
    "    \n",
    "crs/len(pred_reconstructions)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression ratio Rules\n",
    "\n",
    "crs = 0\n",
    "\n",
    "for i in range(len(test_data['originals'])):\n",
    "    token_original = word_tokenize(test_data['originals'][i])\n",
    "    token_prediction = word_tokenize(test_data['predictions'][i])\n",
    "    cr = len(token_prediction)/len(token_original)\n",
    "    crs += cr\n",
    "    \n",
    "crs/len(pred_reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover originals for LSTM\n",
    "\n",
    "originals = []\n",
    "\n",
    "for i in range(len(test_data['predictions'])):\n",
    "    original = test_data['wholeInput'][i]\n",
    "    if len(original) != 0:\n",
    "        if str(original)[0:2] == \"b'\" and str(original)[-1] == \"'\":\n",
    "            original = str(original)[2:-1]\n",
    "            originals.append(original)\n",
    "        elif str(original)[0:2] == 'b\"' and str(original)[-1] == '\"':\n",
    "            original = str(original)[2:-1]\n",
    "            originals.append(original)\n",
    "            \n",
    "# Compression ratio LSTM\n",
    "\n",
    "crs = 0\n",
    "\n",
    "for i in range(len(originals)):\n",
    "    token_original = word_tokenize(originals[i])\n",
    "    token_prediction = word_tokenize(pred_reconstructions[i])\n",
    "    cr = len(token_prediction)/len(token_original)\n",
    "    crs += cr\n",
    "    \n",
    "crs/len(pred_reconstructions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
