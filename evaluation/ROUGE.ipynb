{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE script\n",
    "\n",
    "A script to run the ROUGE evaluation metric. Model outputs must be reconstructed before the ROUGE score is calculated and the script provides different reconstruction functions depending on the model which is being evaluated.\n",
    "\n",
    "ROUGE is implemented using the easy-rouge library: <https://pypi.org/project/easy-rouge>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from rouge.rouge import rouge_n_sentence_level\n",
    "from rouge.rouge import rouge_l_sentence_level\n",
    "from LSTM_reconstruct import LSTM_reconstruct\n",
    "from BERT_reconstruct import BERT_reconstruct\n",
    "from BERT_reconstruct import BERT_rules_reconstruct\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data\n",
    "\n",
    "def read_data(test_file):\n",
    "    with open(test_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"\" # Path to test output\n",
    "test_data = read_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output is from a BERT_rules ensemble model: reconstruct those outputs\n",
    "\n",
    "rules_data = read_data(\"\") # Path to rules output\n",
    "\n",
    "pred_reconstructions, target_reconstructions, originals = BERT_rules_reconstruct(test_data, rules_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output is from a LSTM model: reconstruct those outputs\n",
    "\n",
    "target_reconstructions = []\n",
    "pred_reconstructions = []\n",
    "\n",
    "for i in range(len(test_data['predictions'])):\n",
    "    pred_recon, target_recon = LSTM_reconstruct(test_data['predictions'][i],\n",
    "                                           test_data['targets'][i])\n",
    "    pred_reconstructions.append(pred_recon)\n",
    "    target_reconstructions.append(target_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output is from a BERT model: reconstruct those outputs\n",
    "\n",
    "pred_reconstructions, target_reconstructions, _ = BERT_reconstruct(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output is from a Rules based model: reconstruct those outputs\n",
    "\n",
    "target_reconstructions = []\n",
    "pred_reconstructions = []\n",
    "\n",
    "for i in range(len(test_data['predictions'])):\n",
    "    target_reconstructions.append(test_data['targets'][i])\n",
    "    pred_reconstructions.append(test_data['predictions'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average ROUGE-n\n",
    "\n",
    "count = len(pred_reconstructions)\n",
    "rolling_ROUGEn_r = 0\n",
    "rolling_ROUGEn_p = 0\n",
    "rolling_ROUGEn_f = 0\n",
    "\n",
    "for i in range(len(pred_reconstructions)):\n",
    "    token_target = word_tokenize(target_reconstructions[i])\n",
    "    token_prediction = word_tokenize(pred_reconstructions[i])\n",
    "    recall, precision, rouge = rouge_n_sentence_level(token_prediction, token_target, 2)\n",
    "    rolling_ROUGEn_r += recall\n",
    "    rolling_ROUGEn_p += precision\n",
    "    rolling_ROUGEn_f += rouge\n",
    "    \n",
    "ROUGEn_r = rolling_ROUGEn_r/count\n",
    "ROUGEn_p = rolling_ROUGEn_p/count\n",
    "ROUGEn_f = rolling_ROUGEn_f/count\n",
    "\n",
    "print(ROUGEn_r)\n",
    "print(ROUGEn_p)\n",
    "print(ROUGEn_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average ROUGE-l\n",
    "\n",
    "count = len(pred_reconstructions)\n",
    "rolling_ROUGEl_r = 0\n",
    "rolling_ROUGEl_p = 0\n",
    "rolling_ROUGEl_f = 0\n",
    "\n",
    "for i in range(len(pred_reconstructions)):\n",
    "    token_target = word_tokenize(target_reconstructions[i])\n",
    "    token_prediction = word_tokenize(pred_reconstructions[i])\n",
    "    recall, precision, rouge = rouge_l_sentence_level(token_prediction, token_target)\n",
    "    rolling_ROUGEl_r += recall\n",
    "    rolling_ROUGEl_p += precision\n",
    "    rolling_ROUGEl_f += rouge\n",
    "    \n",
    "ROUGEl_r = rolling_ROUGEl_r/count\n",
    "ROUGEl_p = rolling_ROUGEl_p/count\n",
    "ROUGEl_f = rolling_ROUGEl_f/count\n",
    "\n",
    "print(ROUGEl_r)\n",
    "print(ROUGEl_p)\n",
    "print(ROUGEl_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
